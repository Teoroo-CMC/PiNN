<!--
  This Basic theme serves as an example for how to create other
  themes by demonstrating the features with minimal HTML and CSS.
  Comments like this will be through the code to explain briefly
  what each feature is and point you to the MkDocs documentation
  to find out more.
-->
<!DOCTYPE html>
<head>
  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <!--
    The page_title contains the title for a page as shown in the navigation.
    Site name contains the name as defined in the mkdocs.yml
  -->
  <title>PiNet - PiNN</title>

  <link rel="stylesheet" href="../../css/theme.css">
  <link rel="stylesheet" href="../../css/notebook.css">
  <link rel="stylesheet" href="../../css/pygments.css">
  <link rel="icon" href="data:,">
  
    <link href="../../assets/_mkdocstrings.css" rel="stylesheet">
  
    <link href="../../css/extra.css" rel="stylesheet">
  
    <link href="../../css/ansi-colours.css" rel="stylesheet">
  
    <link href="../../css/jupyter-cells.css" rel="stylesheet">
  
    <link href="../../css/pandas-dataframe.css" rel="stylesheet">
  

  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Source+Code+Pro:ital,wght@0,400;0,600;1,400;1,700&family=Noto+Sans:ital,wght@0,400;0,600;1,400;1,600&display=swap" rel="stylesheet">
  <script
  src="https://code.jquery.com/jquery-3.3.1.slim.min.js"
  integrity="sha256-3edrmyuQ0w65f8gfBsqowzjJe2iM6n0nKciPUp8y+7E="
  crossorigin="anonymous"></script>

  <script>
    var base_url = "../..";
    $(document).ready(function(){
          $('table').wrap('<div class="table-wrapper"></div>');
    });
  </script>

  <script src="../../js/mike.js"></script>

  
    <script src="../../js/mathjax.js"></script>
  
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  
    <script src="https://cdn.jsdelivr.net/npm/mathjax@4.0.0-beta.3/tex-chtml-nofont.js"></script>
  

</head>

<body>
  <header class="header">
    <ul>
      <li id="logo">
        <a href="../.."><span>Pi</span>NN</a>
      </li>
      
      
  
  <li  class="tab" >
    <a href="../..">
      Home
    </a>
  </li>

      
      
  
  <li class="active tab" >
    <a href="../overview/">
      Usage
    </a>
  </li>

      
      
  
  <li  class="tab" >
    <a href="../../notebooks/overview/">
      Notebooks
    </a>
  </li>

      
      <li class="tools">
        
          <a id="nav-toggle" class="tool" onclick="nav_toggle()">
        
          <svg><use xlink:href="../../svg/sprite.svg#menu-2"></use></svg>
        </a>
        <script>
           function nav_toggle() {
               var bar = document.getElementById("tab-bar");
               if (bar.style.display === "none") {
                   bar.style.display = "block";
               } else {
                   bar.style.display = "none";
               }
           }
          function nav_reset() {
              var bar = document.getElementById("tab-bar");
              if (window.innerWidth>950){
                  bar.style.display = "block";
              } else {
                  bar.style.display = "none";
              }
          }
          window.onresize = nav_reset;
        </script>
        
        <a class="tool">
          <svg><use xlink:href="../../svg/sprite.svg#tag"></use></svg>
          <div class="version"></div>
        </a>
        
        
        <a class="tool" href="https://github.com/teoroo-cmc/pinn/">
          <svg><use xlink:href="../../svg/sprite.svg#brand-github"></use></svg>
          <span>teoroo-cmc/pinn</span>
        </a>
        
      </li>
    </ul>
  </header>


  <div id="main">
    <div id="tab-bar">
      <ul>
      
       
  
  <li  class="tab" >
    <a href="../..">
      Home
    </a>
  </li>

         
           <div class="nav-bar">
             <ul>
               
             </ul>
           </div>
         
      
       
  
  <li class="active tab" >
    <a href="../overview/">
      Usage
    </a>
  </li>

         
           <div class="nav-bar">
             <ul>
               
                 
                   
    <li >
      <a href="../overview/" class="">
        Overview
      </a>
    </li>

                 
                   
    <li >
      <a href="../quick_start/" class="">
        Quick Start
      </a>
    </li>

                 
                   
  <a class="nav-title" > IO </a>
  
    
    <li >
      <a href="../datasets/" class="">
        Datasets
      </a>
    </li>

  

                 
                   
  <a class="nav-title" > Networks </a>
  
    
    <li >
      <a href="../networks/" class="">
        Overview
      </a>
    </li>

  
    
    <li >
      <a href="../layers/" class="">
        Layers
      </a>
    </li>

  
    
    <li class="active">
      <a href="./" class="">
        PiNet
      </a>
    </li>

  
    
    <li >
      <a href="../bpnn/" class="">
        BPNN
      </a>
    </li>

  

                 
                   
  <a class="nav-title" > Models </a>
  
    
    <li >
      <a href="../models/" class="">
        Overview
      </a>
    </li>

  
    
    <li >
      <a href="../potential/" class="">
        Potential
      </a>
    </li>

  
    
    <li >
      <a href="../dipole/" class="">
        Dipole
      </a>
    </li>

  
    
    <li >
      <a href="../custom_model/" class="">
        Customize
      </a>
    </li>

  

                 
                   
  <a class="nav-title" > CLI </a>
  
    
    <li >
      <a href="../cli/convert/" class="">
        convert
      </a>
    </li>

  
    
    <li >
      <a href="../cli/train/" class="">
        train
      </a>
    </li>

  
    
    <li >
      <a href="../cli/log/" class="">
        log
      </a>
    </li>

  

                 
                   
  <a class="nav-title" > Misc </a>
  
    
    <li >
      <a href="../optimizers/" class="">
        Optimizers
      </a>
    </li>

  
    
    <li >
      <a href="../visualize/" class="">
        Visualize
      </a>
    </li>

  

                 
               
             </ul>
           </div>
         
      
       
  
  <li  class="tab" >
    <a href="../../notebooks/overview/">
      Notebooks
    </a>
  </li>

         
           <div class="nav-bar">
             <ul>
               
             </ul>
           </div>
         
      
      </ul>
    </div>

    <div id="content">
      <h1 id="the-pinet-network">The PiNet network</h1>
<p>The PiNet network implements the network architecture described in our
paper.<sup id="fnref:1"><a class="footnote-ref" href="#fn:1">1</a></sup> The network architecture features the
graph-convolution which recursively generates atomic properties from local
environment. One distinctive feature of PiNet is that the convolution operation
is realized with pairwise functions whose form are determined by the pair,
called pairwise interactions.</p>
<h2 id="network-architecture">Network architecture</h2>
<p>The overall architecture of PiNet is illustrated with the illustration below:</p>
<p><img alt="PiNet architecture" src="../../tikz/pinet.svg" width="780" /></p>
<p>The preprocess part of the network are implemented with shared layers (see
<a href="../layers/">Layers</a>). The graph-convolution (GC) block are further divided
into PI and IP operations, each consists several layers. Those operations are
recursively applied to update the latent variables, and the output is updated
after each iteration (<code>OutLayer</code>).</p>
<p>We classify the latent variables into the atom-centered "properties"
(<span class="arithmatex">\(\mathbb{P}\)</span>) and the pair-wise "interactions" (<span class="arithmatex">\(\mathbb{I}\)</span>) in our notation.
Since the layers that transform <span class="arithmatex">\(\mathbb{P}\)</span> to <span class="arithmatex">\(\mathbb{P}\)</span> or <span class="arithmatex">\(\mathbb{I}\)</span> to
<span class="arithmatex">\(\mathbb{I}\)</span> are usually standard feed-forward neural networks (<code>FFLayer</code>), the
special part of PiNet are <code>PILayer</code> and <code>IPLayers</code>, which transform between
those two types of variables.</p>
<p>We use the superscript to identify each tensor, and the subscripts to
differentiate the indices of different types for each variable, following the
convention:</p>
<ul>
<li><span class="arithmatex">\(b\)</span>: basis function index;</li>
<li><span class="arithmatex">\(\alpha,\beta,\gamma,\ldots\)</span>: feature channels;</li>
<li><span class="arithmatex">\(i,j,k,\ldots\)</span>: atom indices;</li>
<li><span class="arithmatex">\(x,y,z\)</span>: Cartesian coordinate indices.</li>
</ul>
<p><span class="arithmatex">\(\mathbb{P}^{t}_{i\alpha}\)</span> thus denote value of the <span class="arithmatex">\(\alpha\)</span>-th channel of the
<span class="arithmatex">\(i\)</span>-th atom in the tensor <span class="arithmatex">\(\mathbb{P}^{t}\)</span>. We always provide all the subscripts
of a given tensor in the equations below, so that the dimensionality of each
tensor is unambiguously implied.</p>
<p>For instance, <span class="arithmatex">\(r_{ij}\)</span> entails a scalar distance defined between
each pair of atoms, indexed by <span class="arithmatex">\(i,j\)</span>; <span class="arithmatex">\(\mathbb{P}_{i\alpha}\)</span> entails the atomic
feature vectors indexed by <span class="arithmatex">\(i\)</span> for the atom, and <span class="arithmatex">\(\alpha\)</span> for the channel. The
equations that explain each of the above layers and the hyperparameters
available for the PiNet network are detailed below.</p>
<h2 id="network-specification">Network specification</h2>
<h3 id="pinetpinet">pinet.PiNet</h3>


<div class="doc doc-object doc-class">




  <div class="doc doc-contents first">
          <p class="doc doc-class-bases">
            Bases: <code><span title="tensorflow.keras.Model">Model</span></code></p>

  
      <p>This class implements the Keras Model for the PiNet network.</p>


  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">



<h4 id="pinn.networks.pinet.PiNet.__init__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">atom_types</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">],</span> <span class="n">rc</span><span class="o">=</span><span class="mf">4.0</span><span class="p">,</span> <span class="n">cutoff_type</span><span class="o">=</span><span class="s1">&#39;f1&#39;</span><span class="p">,</span> <span class="n">basis_type</span><span class="o">=</span><span class="s1">&#39;polynomial&#39;</span><span class="p">,</span> <span class="n">n_basis</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">3.0</span><span class="p">,</span> <span class="n">center</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">pp_nodes</span><span class="o">=</span><span class="p">[</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">],</span> <span class="n">pi_nodes</span><span class="o">=</span><span class="p">[</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">],</span> <span class="n">ii_nodes</span><span class="o">=</span><span class="p">[</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">],</span> <span class="n">out_nodes</span><span class="o">=</span><span class="p">[</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">],</span> <span class="n">out_units</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">out_pool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">act</span><span class="o">=</span><span class="s1">&#39;tanh&#39;</span><span class="p">,</span> <span class="n">depth</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span></code>

</h4>


  <div class="doc doc-contents ">
  



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>atom_types</code></td>
          <td>
                <code>list</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>elements for the one-hot embedding</p>
            </div>
          </td>
          <td>
                <code>[1, 6, 7, 8]</code>
          </td>
        </tr>
        <tr>
          <td><code>pp_nodes</code></td>
          <td>
                <code>list</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>number of nodes for PPLayer</p>
            </div>
          </td>
          <td>
                <code>[16, 16]</code>
          </td>
        </tr>
        <tr>
          <td><code>pi_nodes</code></td>
          <td>
                <code>list</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>number of nodes for PILayer</p>
            </div>
          </td>
          <td>
                <code>[16, 16]</code>
          </td>
        </tr>
        <tr>
          <td><code>ii_nodes</code></td>
          <td>
                <code>list</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>number of nodes for IILayer</p>
            </div>
          </td>
          <td>
                <code>[16, 16]</code>
          </td>
        </tr>
        <tr>
          <td><code>out_nodes</code></td>
          <td>
                <code>list</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>number of nodes for OutLayer</p>
            </div>
          </td>
          <td>
                <code>[16, 16]</code>
          </td>
        </tr>
        <tr>
          <td><code>out_pool</code></td>
          <td>
                <code>str</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>pool atomic outputs, see ANNOutput</p>
            </div>
          </td>
          <td>
                <code>False</code>
          </td>
        </tr>
        <tr>
          <td><code>depth</code></td>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>number of interaction blocks</p>
            </div>
          </td>
          <td>
                <code>4</code>
          </td>
        </tr>
        <tr>
          <td><code>rc</code></td>
          <td>
                <code>float</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>cutoff radius</p>
            </div>
          </td>
          <td>
                <code>4.0</code>
          </td>
        </tr>
        <tr>
          <td><code>basis_type</code></td>
          <td>
                <code>string</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>basis function, can be "polynomial" or "gaussian"</p>
            </div>
          </td>
          <td>
                <code>&#39;polynomial&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>n_basis</code></td>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>number of basis functions to use</p>
            </div>
          </td>
          <td>
                <code>4</code>
          </td>
        </tr>
        <tr>
          <td><code>gamma</code></td>
          <td>
                <code>float | array</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>width of gaussian function for gaussian basis</p>
            </div>
          </td>
          <td>
                <code>3.0</code>
          </td>
        </tr>
        <tr>
          <td><code>center</code></td>
          <td>
                <code>float | array</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>center of gaussian function for gaussian basis</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>cutoff_type</code></td>
          <td>
                <code>string</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>cutoff function to use with the basis.</p>
            </div>
          </td>
          <td>
                <code>&#39;f1&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>act</code></td>
          <td>
                <code>string</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>activation function to use</p>
            </div>
          </td>
          <td>
                <code>&#39;tanh&#39;</code>
          </td>
        </tr>
    </tbody>
  </table>

  </div>

</div>


<div class="doc doc-object doc-function">



<h4 id="pinn.networks.pinet.PiNet.call" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">call</span><span class="p">(</span><span class="n">tensors</span><span class="p">)</span></code>

</h4>


  <div class="doc doc-contents ">
  
      <p>PiNet takes batches atomic data as input, the following keys are
required in the input dictionary of tensors:</p>
<ul>
<li><code>ind_1</code>: <a href="../layers/#sparse-indices">sparse indices</a> for the batched data, with shape <code>(n_atoms, 1)</code>;</li>
<li><code>elems</code>: element (atomic numbers) for each atom, with shape <code>(n_atoms)</code>;</li>
<li><code>coord</code>: coordintaes for each atom, with shape <code>(n_atoms, 3)</code>.</li>
</ul>
<p>Optionally, the input dataset can be processed with
<code>PiNet.preprocess(tensors)</code>, which adds the following tensors to the
dictionary:</p>
<ul>
<li><code>ind_2</code>: <a href="../layers/#sparse-indices">sparse indices</a> for neighbour list, with shape <code>(n_pairs, 2)</code>;</li>
<li><code>dist</code>: distances from the neighbour list, with shape <code>(n_pairs)</code>;</li>
<li><code>diff</code>: distance vectors from the neighbour list, with shape <code>(n_pairs, 3)</code>;</li>
<li><code>prop</code>: initial properties <code>(n_pairs, n_elems)</code>;</li>
</ul>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>tensors</code></td>
          <td>
                <code>dict of tensors</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>input tensors</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>output</code></td>          <td>
                <code>tensor</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>output tensor with shape <code>[n_atoms, out_nodes]</code></p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

  </div>

</div>



  </div>

  </div>


</div><h2 id="layer-specifications">Layer specifications</h2>
<h3 id="pinetfflayer">pinet.FFLayer</h3>


<div class="doc doc-object doc-class">




  <div class="doc doc-contents first">
          <p class="doc doc-class-bases">
            Bases: <code><span title="tensorflow.keras.layers.Layer">Layer</span></code></p>

  
      <p><code>FFLayer</code> is a shortcut to create a multi-layer perceptron (MLP) or a
feed-forward network. A <code>FFLayer</code> takes one tensor as input of arbitratry
shape, and parse it to a list of <code>tf.keras.layers.Dense</code> layers, specified
by <code>n_nodes</code>. Each dense layer transforms the input variable as:</p>
<div class="arithmatex">\[
\begin{aligned}
\mathbb{X}'_{\ldots{}\beta} &amp;= \mathrm{Dense}(\mathbb{X}_{\ldots{}\alpha}) \\
  &amp;= h\left( \sum_\alpha W_{\alpha\beta} \mathbb{X}_{\ldots{}\alpha} + b_{\beta} \right)
\end{aligned}
\]</div>
<p>, where <span class="arithmatex">\(W_{\alpha\beta}\)</span>, <span class="arithmatex">\(b_{\beta}\)</span> are the learnable weights and biases,
<span class="arithmatex">\(h\)</span> is the activation function, and <span class="arithmatex">\(\mathbb{X}\)</span> can be
<span class="arithmatex">\(\mathbb{P}_{i\alpha}\)</span> or <span class="arithmatex">\(\mathbb{I}_{ij\alpha}\)</span> with <span class="arithmatex">\(\alpha,\beta\)</span> being
the indices of input/output channels. The keyward arguments are parsed into
the class, which can be used to specify the bias, activation function, etc
for the dense layer. <code>FFLayer</code> outputs a tensor with the shape <code>[...,
n_nodes[-1]]</code>.</p>
<p>In the PiNet architecture, <code>PPLayer</code> and <code>IILayer</code> are both instances of the
<code>FFLayer</code> class , namely:</p>
<div class="arithmatex">\[
\begin{aligned}
  \mathbb{I}_{ij\gamma} &amp;= \mathrm{IILayer}(\mathbb{I}'_{ij\beta}) = \mathrm{FFLayer}(\mathbb{I}'_{ij\beta}) \\
  \mathbb{P}_{i\delta} &amp;= \mathrm{PPLayer}(\mathbb{P}''_{i\gamma}) = \mathrm{FFLayer}(\mathbb{P}'_{i\gamma})
\end{aligned}
\]</div>
<p>, with the difference that <code>IILayer</code>s have their baises set to zero to avoid
discontinuity in the model output.</p>


  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">



<h4 id="pinn.networks.pinet.FFLayer.__init__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">n_nodes</span><span class="o">=</span><span class="p">[</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

</h4>


  <div class="doc doc-contents ">
  



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>n_nodes</code></td>
          <td>
                <code>list</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>dimension of the layers</p>
            </div>
          </td>
          <td>
                <code>[64, 64]</code>
          </td>
        </tr>
        <tr>
          <td><code>**kwargs</code></td>
          <td>
                <code>dict</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>options to be parsed to dense layers</p>
            </div>
          </td>
          <td>
                <code>{}</code>
          </td>
        </tr>
    </tbody>
  </table>

  </div>

</div>


<div class="doc doc-object doc-function">



<h4 id="pinn.networks.pinet.FFLayer.call" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">call</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span></code>

</h4>


  <div class="doc doc-contents ">
  



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>tensor</code></td>
          <td>
                <code>tensor</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>input tensor</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>tensor</code></td>          <td>
                <code>tensor</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>tensor with shape <code>(...,n_nodes[-1])</code></p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

  </div>

</div>



  </div>

  </div>


</div><h3 id="pinetpilayer">pinet.PILayer</h3>


<div class="doc doc-object doc-class">




  <div class="doc doc-contents first">
          <p class="doc doc-class-bases">
            Bases: <code><span title="tensorflow.keras.layers.Layer">Layer</span></code></p>

  
      <p><code>PILayer</code> takes the properties (<span class="arithmatex">\(\mathbb{P}_{i\alpha},
\mathbb{P}_{j\alpha}\)</span>) of a pair of atoms as input and outputs a set of
interactions for each pair. The inputs will be broadcasted and concatenated
as the input of a feed-forward neural network (<code>FFLayer</code>), and the
interactions are generated by taking the output of the <code>FFLayer</code> as weights
of radial basis functions, i.e.:</p>
<div class="arithmatex">\[
\begin{aligned}
w_{ij(b\beta)} &amp;= \mathrm{FFLayer}\left((\mathbf{1}_{j}\mathbb{P}_{i\alpha})\Vert(\mathbf{1}_{i}\mathbb{P}_{j\alpha})\right) \\
\mathbb{I}'_{ij\beta} &amp;= \sum_b W_{ij(b\beta)} \, e_{ijb}
\end{aligned}
\]</div>
<p>, where <span class="arithmatex">\(w_{ij(b\beta)}\)</span> is an intemediate weight tensor for the
radial basis functions, output by the <code>FFLayer</code>; the output channel is
reshaped into two dimensions, where <span class="arithmatex">\(b\)</span> is the index for the basis function
and <span class="arithmatex">\(d\)</span> is the index for output interaction.</p>
<p><code>n_nodes</code> specifies the number of nodes in the <code>FFLayer</code>. Note that the last
element of n_nodes specifies the number of output channels after applying
the basis function (<span class="arithmatex">\(d\)</span> instead of <span class="arithmatex">\(bd\)</span>), i.e. the output dimension of
FFLayer is <code>[n_pairs,n_nodes[-1]*n_basis]</code>, the output is then summed with
the basis to form the output interaction.</p>


  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">



<h4 id="pinn.networks.pinet.PILayer.__init__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">n_nodes</span><span class="o">=</span><span class="p">[</span><span class="mi">64</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

</h4>


  <div class="doc doc-contents ">
  



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>n_nodes</code></td>
          <td>
                <code>list of int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>number of nodes to use</p>
            </div>
          </td>
          <td>
                <code>[64]</code>
          </td>
        </tr>
        <tr>
          <td><code>**kwargs</code></td>
          <td>
                <code>dict</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>keyword arguments will be parsed to the feed forward layers</p>
            </div>
          </td>
          <td>
                <code>{}</code>
          </td>
        </tr>
    </tbody>
  </table>

  </div>

</div>


<div class="doc doc-object doc-function">



<h4 id="pinn.networks.pinet.PILayer.build" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">build</span><span class="p">(</span><span class="n">shapes</span><span class="p">)</span></code>

</h4>


  <div class="doc doc-contents ">
  
      

  </div>

</div>


<div class="doc doc-object doc-function">



<h4 id="pinn.networks.pinet.PILayer.call" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">call</span><span class="p">(</span><span class="n">tensors</span><span class="p">)</span></code>

</h4>


  <div class="doc doc-contents ">
  
      <p>PILayer take a list of three tensors as input:</p>
<ul>
<li>ind_2: <a href="../layers/#sparse-indices">sparse indices</a> of pairs with shape <code>(n_pairs, 2)</code></li>
<li>prop: property tensor with shape <code>(n_atoms, n_prop)</code></li>
<li>basis: interaction tensor with shape <code>(n_pairs, n_basis)</code></li>
</ul>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>tensors</code></td>
          <td>
                <code>list of tensors</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>list of <code>[ind_2, prop, basis]</code> tensors</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>inter</code></td>          <td>
                <code>tensor</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>interaction tensor with shape <code>(n_pairs, n_nodes[-1])</code></p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

  </div>

</div>



  </div>

  </div>


</div><h3 id="pinetiplayer">pinet.IPLayer</h3>


<div class="doc doc-object doc-class">




  <div class="doc doc-contents first">
          <p class="doc doc-class-bases">
            Bases: <code><span title="tensorflow.keras.layers.Layer">Layer</span></code></p>

  
      <p>The IPLayer transforms pairwise interactions to atomic properties</p>
<p>The IPLayer has no learnable variables and simply sums up the pairwise
interations. Thus the returned property has the same shape with the
input interaction, i.e.:</p>
<div class="arithmatex">\[
\begin{aligned}
\mathbb{P}_{i\gamma} = \mathrm{IPLayer}(\mathbb{I}_{ij\gamma}) = \sum_{j} \mathbb{I}_{ij\gamma}
\end{aligned}
\]</div>


  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">



<h4 id="pinn.networks.pinet.IPLayer.__init__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__init__</span><span class="p">()</span></code>

</h4>


  <div class="doc doc-contents ">
  
      <p>IPLayer does not require any parameter, initialize as <code>IPLayer()</code>.</p>

  </div>

</div>


<div class="doc doc-object doc-function">



<h4 id="pinn.networks.pinet.IPLayer.call" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">call</span><span class="p">(</span><span class="n">tensors</span><span class="p">)</span></code>

</h4>


  <div class="doc doc-contents ">
  
      <p>IPLayer take a list of three tensors list as input:</p>
<ul>
<li>ind_2: <a href="../layers/#sparse-indices">sparse indices</a> of pairs with shape <code>(n_pairs, 2)</code></li>
<li>prop: property tensor with shape <code>(n_atoms, n_prop)</code></li>
<li>inter: interaction tensor with shape <code>(n_pairs, n_inter)</code></li>
</ul>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>tensors</code></td>
          <td>
                <code>list of tensor</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>list of [ind_2, prop, inter] tensors</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>prop</code></td>          <td>
                <code>tensor</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>new property tensor with shape <code>(n_atoms, n_inter)</code></p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

  </div>

</div>



  </div>

  </div>


</div><h3 id="pinetresupdate">pinet.ResUpdate</h3>


<div class="doc doc-object doc-class">




  <div class="doc doc-contents first">
          <p class="doc doc-class-bases">
            Bases: <code><span title="tensorflow.keras.layers.Layer">Layer</span></code></p>

  
      <p><code>ResUpdate</code> layer implements ResNet-like update of properties that
addresses vanishing/exploding gradient problems (see
<a href="https://arxiv.org/abs/1512.03385">arXiv:1512.03385</a>).</p>
<p>It takes two tensors (old and new) as input, the tensors should have the
same shape except for the last dimension, and a tensor with the shape of the
new tensor is always returned.</p>
<p>If shapes of the two tensors match, their sum is returned. If the two
tensors' shapes differ in the last dimension, the old tensor will be added
to the new after a learnable linear transformation that matches its shape to
the new tensor, i.e., according to the above flowchart:</p>
<div class="arithmatex">\[
\begin{aligned}
\mathbb{P}'_{i\gamma} &amp;= \mathrm{ResUpdate}(\mathbb{P}^{t}_{i\alpha},\mathbb{P}''_{i\gamma}) &amp; \\
  &amp;= \begin{cases}
       \mathbb{P}^{t}_{i\alpha} + \mathbb{P}''_{i\gamma} &amp; \textrm{, if } \mathrm{dim}(\mathbb{P}^{t}) = \mathrm{dim}(\mathbb{P}'')\\
       \sum_{\alpha} W_{\alpha\gamma} \, \mathbb{P}^{t}_{i\alpha} + \mathbb{P}''_{i\gamma} &amp; \textrm{, if } \mathrm{dim}(\mathbb{P}^{t}) \ne \mathrm{dim}(\mathbb{P}'')
     \end{cases}
\end{aligned}
\]</div>
<p>, where <span class="arithmatex">\(W_{\alpha\beta}\)</span> is a learnable weight matrix if needed.</p>
<p>In the PiNet architecture above, ResUpdate is only used to update the
properties after the <code>IPLayer</code>, when <code>ii_nodes[-1]==pp_nodes[-1]</code>, the
weight matrix is only necessary at <span class="arithmatex">\(t=0\)</span>.</p>


  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">



<h4 id="pinn.networks.pinet.ResUpdate.__init__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__init__</span><span class="p">()</span></code>

</h4>


  <div class="doc doc-contents ">
  
      <p>ResUpdate does not require any parameter, initialize as <code>ResUpdate()</code>.</p>

  </div>

</div>


<div class="doc doc-object doc-function">



<h4 id="pinn.networks.pinet.ResUpdate.build" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">build</span><span class="p">(</span><span class="n">shapes</span><span class="p">)</span></code>

</h4>


  <div class="doc doc-contents ">
  
      

  </div>

</div>


<div class="doc doc-object doc-function">



<h4 id="pinn.networks.pinet.ResUpdate.call" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">call</span><span class="p">(</span><span class="n">tensors</span><span class="p">)</span></code>

</h4>


  <div class="doc doc-contents ">
  



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>tensors</code></td>
          <td>
                <code>list of tensors</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>two tensors with matching shapes expect the last dimension</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>tensor</code></td>          <td>
                <code>tensor</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>updated tensor with the same shape as the second input tensor</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

  </div>

</div>



  </div>

  </div>


</div><h3 id="pinetoutlayer">pinet.OutLayer</h3>


<div class="doc doc-object doc-class">




  <div class="doc doc-contents first">
          <p class="doc doc-class-bases">
            Bases: <code><span title="tensorflow.keras.layers.Layer">Layer</span></code></p>

  
      <p><code>OutLayer</code> updates the network output with a <code>FFLayer</code> layer, where the
<code>out_units</code> controls the dimension of outputs. In addition to the <code>FFLayer</code>
specified by <code>n_nodes</code>, the <code>OutLayer</code> has one additional linear biasless
layer that scales the outputs, specified by <code>out_units</code>.</p>


  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">



<h4 id="pinn.networks.pinet.OutLayer.__init__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">n_nodes</span><span class="p">,</span> <span class="n">out_units</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

</h4>


  <div class="doc doc-contents ">
  



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>n_nodes</code></td>
          <td>
                <code>list</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>dimension of the hidden layers</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>out_units</code></td>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>dimension of the output units</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>**kwargs</code></td>
          <td>
                <code>dict</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>options to be parsed to dense layers</p>
            </div>
          </td>
          <td>
                <code>{}</code>
          </td>
        </tr>
    </tbody>
  </table>

  </div>

</div>


<div class="doc doc-object doc-function">



<h4 id="pinn.networks.pinet.OutLayer.call" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">call</span><span class="p">(</span><span class="n">tensors</span><span class="p">)</span></code>

</h4>


  <div class="doc doc-contents ">
  
      <p>OutLayer takes a list of three tensors as input:</p>
<ul>
<li>ind_1: <a href="../layers/#sparse-indices">sparse indices</a> of atoms with shape <code>(n_atoms, 2)</code></li>
<li>prop: property tensor with shape <code>(n_atoms, n_prop)</code></li>
<li>prev_output:  previous output with shape <code>(n_atoms, out_units)</code></li>
</ul>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>tensors</code></td>
          <td>
                <code>list of tensors</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>list of [ind_1, prop, prev_output] tensors</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
<th>Name</th>        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
<td><code>output</code></td>          <td>
                <code>tensor</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>an updated output tensor with shape <code>(n_atoms, out_units)</code></p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

  </div>

</div>



  </div>

  </div>


</div><div class="footnote">
<hr />
<ol>
<li id="fn:1">
<p><sup>1</sup> Y. Shao, M. Hellström, P.D. Mitev, L. Knijff, and C. Zhang, “<a href="https://doi.org/10.1021/acs.jcim.9b00994">PiNN: A python library for building atomic neural networks of molecules and materials</a>,” J. Chem. Inf. Model. <strong>60</strong>, 1184–1193 (2020).&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>
    </div>

  </div>

  <div class="page">
    <div class="page-prev">
      
      <a href="../layers/" title="Layers"><span>«</span> Previous</a>
      
    </div>
    <div class="page-next">
      
      <a href="../bpnn/" title="BPNN" />Next <span>»</span></a>
      
    </div>
  </div>

  <div class="footer">
    Built with <a href="https://www.mkdocs.org">mkdocs</a> and the <a href="https://github.com/yqshao/mkdocs-flux-theme">flux</a> theme.
  </div>
</body>
</html>