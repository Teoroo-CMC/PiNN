<!--
  This Basic theme serves as an example for how to create other
  themes by demonstrating the features with minimal HTML and CSS.
  Comments like this will be through the code to explain briefly
  what each feature is and point you to the MkDocs documentation
  to find out more.
-->
<!DOCTYPE html>
<head>
  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <!--
    The page_title contains the title for a page as shown in the navigation.
    Site name contains the name as defined in the mkdocs.yml
  -->
  <title>PiNet - PiNN</title>

  <link rel="stylesheet" href="../../css/theme.css">
  <link rel="stylesheet" href="../../css/notebook.css">
  <link rel="stylesheet" href="../../css/pygments.css">
  <link rel="icon" href="data:,">
  
    <link href="../../assets/_mkdocstrings.css" rel="stylesheet">
  
    <link href="../../css/extra.css" rel="stylesheet">
  
    <link href="../../css/ansi-colours.css" rel="stylesheet">
  
    <link href="../../css/jupyter-cells.css" rel="stylesheet">
  
    <link href="../../css/pandas-dataframe.css" rel="stylesheet">
  

  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Source+Code+Pro:ital,wght@0,400;0,600;1,400;1,700&family=Noto+Sans:ital,wght@0,400;0,600;1,400;1,600&display=swap" rel="stylesheet">
  <script
  src="https://code.jquery.com/jquery-3.3.1.slim.min.js"
  integrity="sha256-3edrmyuQ0w65f8gfBsqowzjJe2iM6n0nKciPUp8y+7E="
  crossorigin="anonymous"></script>

  <script>
    var base_url = "../..";
    $(document).ready(function(){
          $('table').wrap('<div class="table-wrapper"></div>');
    });
  </script>

  <script src="../../js/mike.js"></script>

  
    <script src="../../js/mathjax.js"></script>
  
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  
    <script src="https://cdn.jsdelivr.net/npm/mathjax@4.0.0-beta.3/tex-chtml-nofont.js"></script>
  

</head>

<body>
  <header class="header">
    <ul>
      <li id="logo">
        <a href="../.."><span>Pi</span>NN</a>
      </li>
      
      
  
  <li  class="tab" >
    <a href="../..">
      Home
    </a>
  </li>

      
      
  
  <li class="active tab" >
    <a href="../overview/">
      Usage
    </a>
  </li>

      
      
  
  <li  class="tab" >
    <a href="../../notebooks/overview/">
      Notebooks
    </a>
  </li>

      
      <li class="tools">
        
          <a id="nav-toggle" class="tool" onclick="nav_toggle()">
        
          <svg><use xlink:href="../../svg/sprite.svg#menu-2"></use></svg>
        </a>
        <script>
           function nav_toggle() {
               var bar = document.getElementById("tab-bar");
               if (bar.style.display === "none") {
                   bar.style.display = "block";
               } else {
                   bar.style.display = "none";
               }
           }
          function nav_reset() {
              var bar = document.getElementById("tab-bar");
              if (window.innerWidth>950){
                  bar.style.display = "block";
              } else {
                  bar.style.display = "none";
              }
          }
          window.onresize = nav_reset;
        </script>
        
        <a class="tool">
          <svg><use xlink:href="../../svg/sprite.svg#tag"></use></svg>
          <div class="version"></div>
        </a>
        
        
        <a class="tool" href="https://github.com/teoroo-cmc/pinn/">
          <svg><use xlink:href="../../svg/sprite.svg#brand-github"></use></svg>
          <span>teoroo-cmc/pinn</span>
        </a>
        
      </li>
    </ul>
  </header>


  <div id="main">
    <div id="tab-bar">
      <ul>
      
       
  
  <li  class="tab" >
    <a href="../..">
      Home
    </a>
  </li>

         
           <div class="nav-bar">
             <ul>
               
             </ul>
           </div>
         
      
       
  
  <li class="active tab" >
    <a href="../overview/">
      Usage
    </a>
  </li>

         
           <div class="nav-bar">
             <ul>
               
                 
                   
    <li >
      <a href="../overview/" class="">
        Overview
      </a>
    </li>

                 
                   
    <li >
      <a href="../quick_start/" class="">
        Quick Start
      </a>
    </li>

                 
                   
  <a class="nav-title" > IO </a>
  
    
    <li >
      <a href="../datasets/" class="">
        Datasets
      </a>
    </li>

  

                 
                   
  <a class="nav-title" > Networks </a>
  
    
    <li >
      <a href="../networks/" class="">
        Overview
      </a>
    </li>

  
    
    <li >
      <a href="../layers/" class="">
        Layers
      </a>
    </li>

  
    
    <li class="active">
      <a href="./" class="">
        PiNet
      </a>
    </li>

  
    
    <li >
      <a href="../pinet2/" class="">
        PiNet2
      </a>
    </li>

  
    
    <li >
      <a href="../bpnn/" class="">
        BPNN
      </a>
    </li>

  

                 
                   
  <a class="nav-title" > Models </a>
  
    
    <li >
      <a href="../models/" class="">
        Overview
      </a>
    </li>

  
    
    <li >
      <a href="../potential/" class="">
        Potential
      </a>
    </li>

  
    
    <li >
      <a href="../dipole/" class="">
        Dipole
      </a>
    </li>

  
    
    <li >
      <a href="../polarizability/" class="">
        Polarizability
      </a>
    </li>

  
    
    <li >
      <a href="../custom_model/" class="">
        Customize
      </a>
    </li>

  

                 
                   
  <a class="nav-title" > CLI </a>
  
    
    <li >
      <a href="../cli/convert/" class="">
        convert
      </a>
    </li>

  
    
    <li >
      <a href="../cli/train/" class="">
        train
      </a>
    </li>

  
    
    <li >
      <a href="../cli/log/" class="">
        log
      </a>
    </li>

  
    
    <li >
      <a href="../cli/report/" class="">
        report
      </a>
    </li>

  

                 
                   
  <a class="nav-title" > Misc </a>
  
    
    <li >
      <a href="../optimizers/" class="">
        Optimizers
      </a>
    </li>

  
    
    <li >
      <a href="../visualize/" class="">
        Visualize
      </a>
    </li>

  

                 
               
             </ul>
           </div>
         
      
       
  
  <li  class="tab" >
    <a href="../../notebooks/overview/">
      Notebooks
    </a>
  </li>

         
           <div class="nav-bar">
             <ul>
               
             </ul>
           </div>
         
      
      </ul>
    </div>

    <div id="content">
      <h1 id="the-pinet-network">The PiNet network</h1>
<p>The PiNet network implements the network architecture described in our
paper.<sup id="fnref:1"><a class="footnote-ref" href="#fn:1">1</a></sup> The network architecture features the
graph-convolution which recursively generates atomic properties from local
environment. One distinctive feature of PiNet is that the convolution operation
is realized with pairwise functions whose form are determined by the pair,
called pairwise interactions.</p>
<h2 id="network-architecture">Network architecture</h2>
<p>The overall architecture of PiNet is illustrated with the illustration below:</p>
<p><img alt="PiNet architecture" src="../../tikz/pinet.svg" width="780" /></p>
<p>The preprocess part of the network are implemented with shared layers (see
<a href="../layers/">Layers</a>). The graph-convolution (GC) block are further divided
into PI and IP operations, each consists several layers. Those operations are
recursively applied to update the latent variables, and the output is updated
after each iteration (<code>OutLayer</code>).</p>
<p>We classify the latent variables into the atom-centered "properties"
(<span class="arithmatex">\(\mathbb{P}\)</span>) and the pair-wise "interactions" (<span class="arithmatex">\(\mathbb{I}\)</span>) in our notation.
Since the layers that transform <span class="arithmatex">\(\mathbb{P}\)</span> to <span class="arithmatex">\(\mathbb{P}\)</span> or <span class="arithmatex">\(\mathbb{I}\)</span> to
<span class="arithmatex">\(\mathbb{I}\)</span> are usually standard feed-forward neural networks (<code>FFLayer</code>), the
special part of PiNet are <code>PILayer</code> and <code>IPLayers</code>, which transform between
those two types of variables.</p>
<p>We use the superscript to identify each tensor, and the subscripts to
differentiate the indices of different types for each variable, following the
convention:</p>
<ul>
<li><span class="arithmatex">\(b\)</span>: basis function index;</li>
<li><span class="arithmatex">\(\alpha,\beta,\gamma,\ldots\)</span>: feature channels;</li>
<li><span class="arithmatex">\(i,j,k,\ldots\)</span>: atom indices;</li>
<li><span class="arithmatex">\(x,y,z\)</span>: Cartesian coordinate indices.</li>
</ul>
<p><span class="arithmatex">\(\mathbb{P}^{t}_{i\alpha}\)</span> thus denote value of the <span class="arithmatex">\(\alpha\)</span>-th channel of the
<span class="arithmatex">\(i\)</span>-th atom in the tensor <span class="arithmatex">\(\mathbb{P}^{t}\)</span>. We always provide all the subscripts
of a given tensor in the equations below, so that the dimensionality of each
tensor is unambiguously implied.</p>
<p>For instance, <span class="arithmatex">\(r_{ij}\)</span> entails a scalar distance defined between
each pair of atoms, indexed by <span class="arithmatex">\(i,j\)</span>; <span class="arithmatex">\(\mathbb{P}_{i\alpha}\)</span> entails the atomic
feature vectors indexed by <span class="arithmatex">\(i\)</span> for the atom, and <span class="arithmatex">\(\alpha\)</span> for the channel. The
equations that explain each of the above layers and the hyperparameters
available for the PiNet network are detailed below.</p>
<p>The parameters for <code>PiNet</code> are outlined in the network specification and can be applied in the configuration file as shown in the following snippet:</p>
<div class="highlight"><pre><span></span><code>&quot;network&quot;: {
    &quot;name&quot;: &quot;PiNet&quot;,
    &quot;params&quot;: {
        &quot;atom_types&quot;: [1, 8],
        &quot;basis_type&quot;: &quot;gaussian&quot;,
        &quot;depth&quot;: 5,
        &quot;ii_nodes&quot;: [16, 16, 16, 16],
        &quot;n_basis&quot;: 10,
        &quot;out_nodes&quot;: [16],
        &quot;pi_nodes&quot;: [16],
        &quot;pp_nodes&quot;: [16, 16, 16, 16],
        &quot;rc&quot;: 6.0,
    }
},
</code></pre></div>
<h2 id="network-specification">Network specification</h2>
<h3 id="pinetpinet">pinet.PiNet</h3>


<div class="doc doc-object doc-class">



<a id="pinn.networks.pinet.PiNet"></a>
    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="tensorflow.keras.Model">Model</span></code></p>


        <p>This class implements the Keras Model for the PiNet network.</p>






              <details class="quote">
                <summary>Source code in <code>pinn/networks/pinet.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">PiNet</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;This class implements the Keras Model for the PiNet network.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">atom_types</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">],</span>
        <span class="n">rc</span><span class="o">=</span><span class="mf">4.0</span><span class="p">,</span>
        <span class="n">cutoff_type</span><span class="o">=</span><span class="s2">&quot;f1&quot;</span><span class="p">,</span>
        <span class="n">basis_type</span><span class="o">=</span><span class="s2">&quot;polynomial&quot;</span><span class="p">,</span>
        <span class="n">n_basis</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
        <span class="n">gamma</span><span class="o">=</span><span class="mf">3.0</span><span class="p">,</span>
        <span class="n">center</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">pp_nodes</span><span class="o">=</span><span class="p">[</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">],</span>
        <span class="n">pi_nodes</span><span class="o">=</span><span class="p">[</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">],</span>
        <span class="n">ii_nodes</span><span class="o">=</span><span class="p">[</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">],</span>
        <span class="n">out_nodes</span><span class="o">=</span><span class="p">[</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">],</span>
        <span class="n">out_units</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">out_pool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">act</span><span class="o">=</span><span class="s2">&quot;tanh&quot;</span><span class="p">,</span>
        <span class="n">depth</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            atom_types (list): elements for the one-hot embedding</span>
<span class="sd">            pp_nodes (list): number of nodes for PPLayer</span>
<span class="sd">            pi_nodes (list): number of nodes for PILayer</span>
<span class="sd">            ii_nodes (list): number of nodes for IILayer</span>
<span class="sd">            out_nodes (list): number of nodes for OutLayer</span>
<span class="sd">            out_pool (str): pool atomic outputs, see ANNOutput</span>
<span class="sd">            depth (int): number of interaction blocks</span>
<span class="sd">            rc (float): cutoff radius</span>
<span class="sd">            basis_type (string): basis function, can be &quot;polynomial&quot; or &quot;gaussian&quot;</span>
<span class="sd">            n_basis (int): number of basis functions to use</span>
<span class="sd">            gamma (float|array): width of gaussian function for gaussian basis</span>
<span class="sd">            center (float|array): center of gaussian function for gaussian basis</span>
<span class="sd">            cutoff_type (string): cutoff function to use with the basis.</span>
<span class="sd">            act (string): activation function to use</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">PiNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">depth</span> <span class="o">=</span> <span class="n">depth</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">preprocess</span> <span class="o">=</span> <span class="n">PreprocessLayer</span><span class="p">(</span><span class="n">atom_types</span><span class="p">,</span> <span class="n">rc</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cutoff</span> <span class="o">=</span> <span class="n">CutoffFunc</span><span class="p">(</span><span class="n">rc</span><span class="p">,</span> <span class="n">cutoff_type</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">basis_type</span> <span class="o">==</span> <span class="s2">&quot;polynomial&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">basis_fn</span> <span class="o">=</span> <span class="n">PolynomialBasis</span><span class="p">(</span><span class="n">n_basis</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">basis_type</span> <span class="o">==</span> <span class="s2">&quot;gaussian&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">basis_fn</span> <span class="o">=</span> <span class="n">GaussianBasis</span><span class="p">(</span><span class="n">center</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">rc</span><span class="p">,</span> <span class="n">n_basis</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">res_update</span> <span class="o">=</span> <span class="p">[</span><span class="n">ResUpdate</span><span class="p">()</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">depth</span><span class="p">)]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gc_blocks</span> <span class="o">=</span> <span class="p">[</span><span class="n">GCBlock</span><span class="p">([],</span> <span class="n">pi_nodes</span><span class="p">,</span> <span class="n">ii_nodes</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">act</span><span class="p">)]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gc_blocks</span> <span class="o">+=</span> <span class="p">[</span>
            <span class="n">GCBlock</span><span class="p">(</span><span class="n">pp_nodes</span><span class="p">,</span> <span class="n">pi_nodes</span><span class="p">,</span> <span class="n">ii_nodes</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">act</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">depth</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
        <span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_layers</span> <span class="o">=</span> <span class="p">[</span><span class="n">OutLayer</span><span class="p">(</span><span class="n">out_nodes</span><span class="p">,</span> <span class="n">out_units</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">depth</span><span class="p">)]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ann_output</span> <span class="o">=</span> <span class="n">ANNOutput</span><span class="p">(</span><span class="n">out_pool</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensors</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;PiNet takes batches atomic data as input, the following keys are</span>
<span class="sd">        required in the input dictionary of tensors:</span>

<span class="sd">        - `ind_1`: [sparse indices](layers.md#sparse-indices) for the batched data, with shape `(n_atoms, 1)`;</span>
<span class="sd">        - `elems`: element (atomic numbers) for each atom, with shape `(n_atoms)`;</span>
<span class="sd">        - `coord`: coordintaes for each atom, with shape `(n_atoms, 3)`.</span>

<span class="sd">        Optionally, the input dataset can be processed with</span>
<span class="sd">        `PiNet.preprocess(tensors)`, which adds the following tensors to the</span>
<span class="sd">        dictionary:</span>

<span class="sd">        - `ind_2`: [sparse indices](layers.md#sparse-indices) for neighbour list, with shape `(n_pairs, 2)`;</span>
<span class="sd">        - `dist`: distances from the neighbour list, with shape `(n_pairs)`;</span>
<span class="sd">        - `diff`: distance vectors from the neighbour list, with shape `(n_pairs, 3)`;</span>
<span class="sd">        - `prop`: initial properties `(n_pairs, n_elems)`;</span>

<span class="sd">        Args:</span>
<span class="sd">            tensors (dict of tensors): input tensors</span>

<span class="sd">        Returns:</span>
<span class="sd">            output (tensor): output tensor with shape `[n_atoms, out_nodes]`</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">tensors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">preprocess</span><span class="p">(</span><span class="n">tensors</span><span class="p">)</span>
        <span class="n">fc</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cutoff</span><span class="p">(</span><span class="n">tensors</span><span class="p">[</span><span class="s2">&quot;dist&quot;</span><span class="p">])</span>
        <span class="n">basis</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">basis_fn</span><span class="p">(</span><span class="n">tensors</span><span class="p">[</span><span class="s2">&quot;dist&quot;</span><span class="p">],</span> <span class="n">fc</span><span class="o">=</span><span class="n">fc</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">depth</span><span class="p">):</span>
            <span class="n">prop</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gc_blocks</span><span class="p">[</span><span class="n">i</span><span class="p">]([</span><span class="n">tensors</span><span class="p">[</span><span class="s2">&quot;ind_2&quot;</span><span class="p">],</span> <span class="n">tensors</span><span class="p">[</span><span class="s2">&quot;prop&quot;</span><span class="p">],</span> <span class="n">basis</span><span class="p">])</span>
            <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]([</span><span class="n">tensors</span><span class="p">[</span><span class="s2">&quot;ind_1&quot;</span><span class="p">],</span> <span class="n">prop</span><span class="p">,</span> <span class="n">output</span><span class="p">])</span>
            <span class="n">tensors</span><span class="p">[</span><span class="s2">&quot;prop&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">res_update</span><span class="p">[</span><span class="n">i</span><span class="p">]([</span><span class="n">tensors</span><span class="p">[</span><span class="s2">&quot;prop&quot;</span><span class="p">],</span> <span class="n">prop</span><span class="p">])</span>

        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ann_output</span><span class="p">([</span><span class="n">tensors</span><span class="p">[</span><span class="s2">&quot;ind_1&quot;</span><span class="p">],</span> <span class="n">output</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">output</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h2 id="pinn.networks.pinet.PiNet.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">atom_types</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">],</span> <span class="n">rc</span><span class="o">=</span><span class="mf">4.0</span><span class="p">,</span> <span class="n">cutoff_type</span><span class="o">=</span><span class="s1">&#39;f1&#39;</span><span class="p">,</span> <span class="n">basis_type</span><span class="o">=</span><span class="s1">&#39;polynomial&#39;</span><span class="p">,</span> <span class="n">n_basis</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">3.0</span><span class="p">,</span> <span class="n">center</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">pp_nodes</span><span class="o">=</span><span class="p">[</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">],</span> <span class="n">pi_nodes</span><span class="o">=</span><span class="p">[</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">],</span> <span class="n">ii_nodes</span><span class="o">=</span><span class="p">[</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">],</span> <span class="n">out_nodes</span><span class="o">=</span><span class="p">[</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">],</span> <span class="n">out_units</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">out_pool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">act</span><span class="o">=</span><span class="s1">&#39;tanh&#39;</span><span class="p">,</span> <span class="n">depth</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span></code>

</h2>


    <div class="doc doc-contents ">



<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>atom_types</code>
            </td>
            <td>
                  <code><span title="list">list</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>elements for the one-hot embedding</p>
              </div>
            </td>
            <td>
                  <code>[1, 6, 7, 8]</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>pp_nodes</code>
            </td>
            <td>
                  <code><span title="list">list</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>number of nodes for PPLayer</p>
              </div>
            </td>
            <td>
                  <code>[16, 16]</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>pi_nodes</code>
            </td>
            <td>
                  <code><span title="list">list</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>number of nodes for PILayer</p>
              </div>
            </td>
            <td>
                  <code>[16, 16]</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>ii_nodes</code>
            </td>
            <td>
                  <code><span title="list">list</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>number of nodes for IILayer</p>
              </div>
            </td>
            <td>
                  <code>[16, 16]</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>out_nodes</code>
            </td>
            <td>
                  <code><span title="list">list</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>number of nodes for OutLayer</p>
              </div>
            </td>
            <td>
                  <code>[16, 16]</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>out_pool</code>
            </td>
            <td>
                  <code><span title="str">str</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>pool atomic outputs, see ANNOutput</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>depth</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>number of interaction blocks</p>
              </div>
            </td>
            <td>
                  <code>4</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>rc</code>
            </td>
            <td>
                  <code><span title="float">float</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>cutoff radius</p>
              </div>
            </td>
            <td>
                  <code>4.0</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>basis_type</code>
            </td>
            <td>
                  <code><span title="string">string</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>basis function, can be "polynomial" or "gaussian"</p>
              </div>
            </td>
            <td>
                  <code>&#39;polynomial&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>n_basis</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>number of basis functions to use</p>
              </div>
            </td>
            <td>
                  <code>4</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>gamma</code>
            </td>
            <td>
                  <code><span title="float">float</span> | <span title="array">array</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>width of gaussian function for gaussian basis</p>
              </div>
            </td>
            <td>
                  <code>3.0</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>center</code>
            </td>
            <td>
                  <code><span title="float">float</span> | <span title="array">array</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>center of gaussian function for gaussian basis</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cutoff_type</code>
            </td>
            <td>
                  <code><span title="string">string</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>cutoff function to use with the basis.</p>
              </div>
            </td>
            <td>
                  <code>&#39;f1&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>act</code>
            </td>
            <td>
                  <code><span title="string">string</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>activation function to use</p>
              </div>
            </td>
            <td>
                  <code>&#39;tanh&#39;</code>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>pinn/networks/pinet.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">atom_types</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">],</span>
    <span class="n">rc</span><span class="o">=</span><span class="mf">4.0</span><span class="p">,</span>
    <span class="n">cutoff_type</span><span class="o">=</span><span class="s2">&quot;f1&quot;</span><span class="p">,</span>
    <span class="n">basis_type</span><span class="o">=</span><span class="s2">&quot;polynomial&quot;</span><span class="p">,</span>
    <span class="n">n_basis</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">gamma</span><span class="o">=</span><span class="mf">3.0</span><span class="p">,</span>
    <span class="n">center</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">pp_nodes</span><span class="o">=</span><span class="p">[</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">],</span>
    <span class="n">pi_nodes</span><span class="o">=</span><span class="p">[</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">],</span>
    <span class="n">ii_nodes</span><span class="o">=</span><span class="p">[</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">],</span>
    <span class="n">out_nodes</span><span class="o">=</span><span class="p">[</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">],</span>
    <span class="n">out_units</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">out_pool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">act</span><span class="o">=</span><span class="s2">&quot;tanh&quot;</span><span class="p">,</span>
    <span class="n">depth</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        atom_types (list): elements for the one-hot embedding</span>
<span class="sd">        pp_nodes (list): number of nodes for PPLayer</span>
<span class="sd">        pi_nodes (list): number of nodes for PILayer</span>
<span class="sd">        ii_nodes (list): number of nodes for IILayer</span>
<span class="sd">        out_nodes (list): number of nodes for OutLayer</span>
<span class="sd">        out_pool (str): pool atomic outputs, see ANNOutput</span>
<span class="sd">        depth (int): number of interaction blocks</span>
<span class="sd">        rc (float): cutoff radius</span>
<span class="sd">        basis_type (string): basis function, can be &quot;polynomial&quot; or &quot;gaussian&quot;</span>
<span class="sd">        n_basis (int): number of basis functions to use</span>
<span class="sd">        gamma (float|array): width of gaussian function for gaussian basis</span>
<span class="sd">        center (float|array): center of gaussian function for gaussian basis</span>
<span class="sd">        cutoff_type (string): cutoff function to use with the basis.</span>
<span class="sd">        act (string): activation function to use</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">PiNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">depth</span> <span class="o">=</span> <span class="n">depth</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">preprocess</span> <span class="o">=</span> <span class="n">PreprocessLayer</span><span class="p">(</span><span class="n">atom_types</span><span class="p">,</span> <span class="n">rc</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">cutoff</span> <span class="o">=</span> <span class="n">CutoffFunc</span><span class="p">(</span><span class="n">rc</span><span class="p">,</span> <span class="n">cutoff_type</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">basis_type</span> <span class="o">==</span> <span class="s2">&quot;polynomial&quot;</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">basis_fn</span> <span class="o">=</span> <span class="n">PolynomialBasis</span><span class="p">(</span><span class="n">n_basis</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">basis_type</span> <span class="o">==</span> <span class="s2">&quot;gaussian&quot;</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">basis_fn</span> <span class="o">=</span> <span class="n">GaussianBasis</span><span class="p">(</span><span class="n">center</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">rc</span><span class="p">,</span> <span class="n">n_basis</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">res_update</span> <span class="o">=</span> <span class="p">[</span><span class="n">ResUpdate</span><span class="p">()</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">depth</span><span class="p">)]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">gc_blocks</span> <span class="o">=</span> <span class="p">[</span><span class="n">GCBlock</span><span class="p">([],</span> <span class="n">pi_nodes</span><span class="p">,</span> <span class="n">ii_nodes</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">act</span><span class="p">)]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">gc_blocks</span> <span class="o">+=</span> <span class="p">[</span>
        <span class="n">GCBlock</span><span class="p">(</span><span class="n">pp_nodes</span><span class="p">,</span> <span class="n">pi_nodes</span><span class="p">,</span> <span class="n">ii_nodes</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">act</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">depth</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    <span class="p">]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">out_layers</span> <span class="o">=</span> <span class="p">[</span><span class="n">OutLayer</span><span class="p">(</span><span class="n">out_nodes</span><span class="p">,</span> <span class="n">out_units</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">depth</span><span class="p">)]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ann_output</span> <span class="o">=</span> <span class="n">ANNOutput</span><span class="p">(</span><span class="n">out_pool</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="pinn.networks.pinet.PiNet.call" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">call</span><span class="p">(</span><span class="n">tensors</span><span class="p">)</span></code>

</h2>


    <div class="doc doc-contents ">

        <p>PiNet takes batches atomic data as input, the following keys are
required in the input dictionary of tensors:</p>
<ul>
<li><code>ind_1</code>: <a href="../layers/#sparse-indices">sparse indices</a> for the batched data, with shape <code>(n_atoms, 1)</code>;</li>
<li><code>elems</code>: element (atomic numbers) for each atom, with shape <code>(n_atoms)</code>;</li>
<li><code>coord</code>: coordintaes for each atom, with shape <code>(n_atoms, 3)</code>.</li>
</ul>
<p>Optionally, the input dataset can be processed with
<code>PiNet.preprocess(tensors)</code>, which adds the following tensors to the
dictionary:</p>
<ul>
<li><code>ind_2</code>: <a href="../layers/#sparse-indices">sparse indices</a> for neighbour list, with shape <code>(n_pairs, 2)</code>;</li>
<li><code>dist</code>: distances from the neighbour list, with shape <code>(n_pairs)</code>;</li>
<li><code>diff</code>: distance vectors from the neighbour list, with shape <code>(n_pairs, 3)</code>;</li>
<li><code>prop</code>: initial properties <code>(n_pairs, n_elems)</code>;</li>
</ul>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>tensors</code>
            </td>
            <td>
                  <code>dict of tensors</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>input tensors</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>output</code></td>            <td>
                  <code><span title="tensor">tensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>output tensor with shape <code>[n_atoms, out_nodes]</code></p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>pinn/networks/pinet.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensors</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;PiNet takes batches atomic data as input, the following keys are</span>
<span class="sd">    required in the input dictionary of tensors:</span>

<span class="sd">    - `ind_1`: [sparse indices](layers.md#sparse-indices) for the batched data, with shape `(n_atoms, 1)`;</span>
<span class="sd">    - `elems`: element (atomic numbers) for each atom, with shape `(n_atoms)`;</span>
<span class="sd">    - `coord`: coordintaes for each atom, with shape `(n_atoms, 3)`.</span>

<span class="sd">    Optionally, the input dataset can be processed with</span>
<span class="sd">    `PiNet.preprocess(tensors)`, which adds the following tensors to the</span>
<span class="sd">    dictionary:</span>

<span class="sd">    - `ind_2`: [sparse indices](layers.md#sparse-indices) for neighbour list, with shape `(n_pairs, 2)`;</span>
<span class="sd">    - `dist`: distances from the neighbour list, with shape `(n_pairs)`;</span>
<span class="sd">    - `diff`: distance vectors from the neighbour list, with shape `(n_pairs, 3)`;</span>
<span class="sd">    - `prop`: initial properties `(n_pairs, n_elems)`;</span>

<span class="sd">    Args:</span>
<span class="sd">        tensors (dict of tensors): input tensors</span>

<span class="sd">    Returns:</span>
<span class="sd">        output (tensor): output tensor with shape `[n_atoms, out_nodes]`</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">tensors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">preprocess</span><span class="p">(</span><span class="n">tensors</span><span class="p">)</span>
    <span class="n">fc</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cutoff</span><span class="p">(</span><span class="n">tensors</span><span class="p">[</span><span class="s2">&quot;dist&quot;</span><span class="p">])</span>
    <span class="n">basis</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">basis_fn</span><span class="p">(</span><span class="n">tensors</span><span class="p">[</span><span class="s2">&quot;dist&quot;</span><span class="p">],</span> <span class="n">fc</span><span class="o">=</span><span class="n">fc</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">depth</span><span class="p">):</span>
        <span class="n">prop</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gc_blocks</span><span class="p">[</span><span class="n">i</span><span class="p">]([</span><span class="n">tensors</span><span class="p">[</span><span class="s2">&quot;ind_2&quot;</span><span class="p">],</span> <span class="n">tensors</span><span class="p">[</span><span class="s2">&quot;prop&quot;</span><span class="p">],</span> <span class="n">basis</span><span class="p">])</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]([</span><span class="n">tensors</span><span class="p">[</span><span class="s2">&quot;ind_1&quot;</span><span class="p">],</span> <span class="n">prop</span><span class="p">,</span> <span class="n">output</span><span class="p">])</span>
        <span class="n">tensors</span><span class="p">[</span><span class="s2">&quot;prop&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">res_update</span><span class="p">[</span><span class="n">i</span><span class="p">]([</span><span class="n">tensors</span><span class="p">[</span><span class="s2">&quot;prop&quot;</span><span class="p">],</span> <span class="n">prop</span><span class="p">])</span>

    <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ann_output</span><span class="p">([</span><span class="n">tensors</span><span class="p">[</span><span class="s2">&quot;ind_1&quot;</span><span class="p">],</span> <span class="n">output</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">output</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div><h2 id="layer-specifications">Layer specifications</h2>
<h3 id="pinetfflayer">pinet.FFLayer</h3>


<div class="doc doc-object doc-class">



<a id="pinn.networks.pinet.FFLayer"></a>
    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="tensorflow.keras.layers.Layer">Layer</span></code></p>


        <p><code>FFLayer</code> is a shortcut to create a multi-layer perceptron (MLP) or a
feed-forward network. A <code>FFLayer</code> takes one tensor as input of arbitratry
shape, and parse it to a list of <code>tf.keras.layers.Dense</code> layers, specified
by <code>n_nodes</code>. Each dense layer transforms the input variable as:</p>
<div class="arithmatex">\[
\begin{aligned}
\mathbb{X}'_{\ldots{}\beta} &amp;= \mathrm{Dense}(\mathbb{X}_{\ldots{}\alpha}) \\
  &amp;= h\left( \sum_\alpha W_{\alpha\beta} \mathbb{X}_{\ldots{}\alpha} + b_{\beta} \right)
\end{aligned}
\]</div>
<p>, where <span class="arithmatex">\(W_{\alpha\beta}\)</span>, <span class="arithmatex">\(b_{\beta}\)</span> are the learnable weights and biases,
<span class="arithmatex">\(h\)</span> is the activation function, and <span class="arithmatex">\(\mathbb{X}\)</span> can be
<span class="arithmatex">\(\mathbb{P}_{i\alpha}\)</span> or <span class="arithmatex">\(\mathbb{I}_{ij\alpha}\)</span> with <span class="arithmatex">\(\alpha,\beta\)</span> being
the indices of input/output channels. The keyward arguments are parsed into
the class, which can be used to specify the bias, activation function, etc
for the dense layer. <code>FFLayer</code> outputs a tensor with the shape <code>[...,
n_nodes[-1]]</code>.</p>
<p>In the PiNet architecture, <code>PPLayer</code> and <code>IILayer</code> are both instances of the
<code>FFLayer</code> class , namely:</p>
<div class="arithmatex">\[
\begin{aligned}
  \mathbb{I}_{ij\gamma} &amp;= \mathrm{IILayer}(\mathbb{I}'_{ij\beta}) = \mathrm{FFLayer}(\mathbb{I}'_{ij\beta}) \\
  \mathbb{P}_{i\delta} &amp;= \mathrm{PPLayer}(\mathbb{P}''_{i\gamma}) = \mathrm{FFLayer}(\mathbb{P}'_{i\gamma})
\end{aligned}
\]</div>
<p>, with the difference that <code>IILayer</code>s have their baises set to zero to avoid
discontinuity in the model output.</p>






              <details class="quote">
                <summary>Source code in <code>pinn/networks/pinet.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">FFLayer</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">R</span><span class="sd">&quot;&quot;&quot;`FFLayer` is a shortcut to create a multi-layer perceptron (MLP) or a</span>
<span class="sd">    feed-forward network. A `FFLayer` takes one tensor as input of arbitratry</span>
<span class="sd">    shape, and parse it to a list of `tf.keras.layers.Dense` layers, specified</span>
<span class="sd">    by `n_nodes`. Each dense layer transforms the input variable as:</span>

<span class="sd">    $$</span>
<span class="sd">    \begin{aligned}</span>
<span class="sd">    \mathbb{X}&#39;_{\ldots{}\beta} &amp;= \mathrm{Dense}(\mathbb{X}_{\ldots{}\alpha}) \\</span>
<span class="sd">      &amp;= h\left( \sum_\alpha W_{\alpha\beta} \mathbb{X}_{\ldots{}\alpha} + b_{\beta} \right)</span>
<span class="sd">    \end{aligned}</span>
<span class="sd">    $$</span>

<span class="sd">    , where $W_{\alpha\beta}$, $b_{\beta}$ are the learnable weights and biases,</span>
<span class="sd">    $h$ is the activation function, and $\mathbb{X}$ can be</span>
<span class="sd">    $\mathbb{P}_{i\alpha}$ or $\mathbb{I}_{ij\alpha}$ with $\alpha,\beta$ being</span>
<span class="sd">    the indices of input/output channels. The keyward arguments are parsed into</span>
<span class="sd">    the class, which can be used to specify the bias, activation function, etc</span>
<span class="sd">    for the dense layer. `FFLayer` outputs a tensor with the shape `[...,</span>
<span class="sd">    n_nodes[-1]]`.</span>


<span class="sd">    In the PiNet architecture, `PPLayer` and `IILayer` are both instances of the</span>
<span class="sd">    `FFLayer` class , namely:</span>

<span class="sd">    $$</span>
<span class="sd">    \begin{aligned}</span>
<span class="sd">      \mathbb{I}_{ij\gamma} &amp;= \mathrm{IILayer}(\mathbb{I}&#39;_{ij\beta}) = \mathrm{FFLayer}(\mathbb{I}&#39;_{ij\beta}) \\</span>
<span class="sd">      \mathbb{P}_{i\delta} &amp;= \mathrm{PPLayer}(\mathbb{P}&#39;&#39;_{i\gamma}) = \mathrm{FFLayer}(\mathbb{P}&#39;_{i\gamma})</span>
<span class="sd">    \end{aligned}</span>
<span class="sd">    $$</span>

<span class="sd">    , with the difference that `IILayer`s have their baises set to zero to avoid</span>
<span class="sd">    discontinuity in the model output.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_nodes</span><span class="o">=</span><span class="p">[</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            n_nodes (list): dimension of the layers</span>
<span class="sd">            **kwargs (dict): options to be parsed to dense layers</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">FFLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense_layers</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">n_node</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="k">for</span> <span class="n">n_node</span> <span class="ow">in</span> <span class="n">n_nodes</span>
        <span class="p">]</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            tensor (tensor): input tensor</span>

<span class="sd">        Returns:</span>
<span class="sd">            tensor (tensor): tensor with shape `(...,n_nodes[-1])`</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense_layers</span><span class="p">:</span>
            <span class="n">tensor</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">tensor</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h2 id="pinn.networks.pinet.FFLayer.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">n_nodes</span><span class="o">=</span><span class="p">[</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

</h2>


    <div class="doc doc-contents ">



<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>n_nodes</code>
            </td>
            <td>
                  <code><span title="list">list</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>dimension of the layers</p>
              </div>
            </td>
            <td>
                  <code>[64, 64]</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>**kwargs</code>
            </td>
            <td>
                  <code><span title="dict">dict</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>options to be parsed to dense layers</p>
              </div>
            </td>
            <td>
                  <code>{}</code>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>pinn/networks/pinet.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_nodes</span><span class="o">=</span><span class="p">[</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        n_nodes (list): dimension of the layers</span>
<span class="sd">        **kwargs (dict): options to be parsed to dense layers</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">FFLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dense_layers</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">n_node</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="k">for</span> <span class="n">n_node</span> <span class="ow">in</span> <span class="n">n_nodes</span>
    <span class="p">]</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="pinn.networks.pinet.FFLayer.call" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">call</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span></code>

</h2>


    <div class="doc doc-contents ">



<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>tensor</code>
            </td>
            <td>
                  <code><span title="tensor">tensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>input tensor</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>tensor</code></td>            <td>
                  <code><span title="tensor">tensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>tensor with shape <code>(...,n_nodes[-1])</code></p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>pinn/networks/pinet.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        tensor (tensor): input tensor</span>

<span class="sd">    Returns:</span>
<span class="sd">        tensor (tensor): tensor with shape `(...,n_nodes[-1])`</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense_layers</span><span class="p">:</span>
        <span class="n">tensor</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tensor</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div><h3 id="pinetpilayer">pinet.PILayer</h3>


<div class="doc doc-object doc-class">



<a id="pinn.networks.pinet.PILayer"></a>
    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="tensorflow.keras.layers.Layer">Layer</span></code></p>


        <p><code>PILayer</code> takes the properties (<span class="arithmatex">\(\mathbb{P}_{i\alpha},
\mathbb{P}_{j\alpha}\)</span>) of a pair of atoms as input and outputs a set of
interactions for each pair. The inputs will be broadcasted and concatenated
as the input of a feed-forward neural network (<code>FFLayer</code>), and the
interactions are generated by taking the output of the <code>FFLayer</code> as weights
of radial basis functions, i.e.:</p>
<div class="arithmatex">\[
\begin{aligned}
w_{ij(b\beta)} &amp;= \mathrm{FFLayer}\left((\mathbf{1}_{j}\mathbb{P}_{i\alpha})\Vert(\mathbf{1}_{i}\mathbb{P}_{j\alpha})\right) \\
\mathbb{I}'_{ij\beta} &amp;= \sum_b W_{ij(b\beta)} \, e_{ijb}
\end{aligned}
\]</div>
<p>, where <span class="arithmatex">\(w_{ij(b\beta)}\)</span> is an intemediate weight tensor for the
radial basis functions, output by the <code>FFLayer</code>; the output channel is
reshaped into two dimensions, where <span class="arithmatex">\(b\)</span> is the index for the basis function
and <span class="arithmatex">\(d\)</span> is the index for output interaction.</p>
<p><code>n_nodes</code> specifies the number of nodes in the <code>FFLayer</code>. Note that the last
element of n_nodes specifies the number of output channels after applying
the basis function (<span class="arithmatex">\(d\)</span> instead of <span class="arithmatex">\(bd\)</span>), i.e. the output dimension of
FFLayer is <code>[n_pairs,n_nodes[-1]*n_basis]</code>, the output is then summed with
the basis to form the output interaction.</p>






              <details class="quote">
                <summary>Source code in <code>pinn/networks/pinet.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">PILayer</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">R</span><span class="sd">&quot;&quot;&quot;`PILayer` takes the properties ($\mathbb{P}_{i\alpha},</span>
<span class="sd">    \mathbb{P}_{j\alpha}$) of a pair of atoms as input and outputs a set of</span>
<span class="sd">    interactions for each pair. The inputs will be broadcasted and concatenated</span>
<span class="sd">    as the input of a feed-forward neural network (`FFLayer`), and the</span>
<span class="sd">    interactions are generated by taking the output of the `FFLayer` as weights</span>
<span class="sd">    of radial basis functions, i.e.:</span>

<span class="sd">    $$</span>
<span class="sd">    \begin{aligned}</span>
<span class="sd">    w_{ij(b\beta)} &amp;= \mathrm{FFLayer}\left((\mathbf{1}_{j}\mathbb{P}_{i\alpha})\Vert(\mathbf{1}_{i}\mathbb{P}_{j\alpha})\right) \\</span>
<span class="sd">    \mathbb{I}&#39;_{ij\beta} &amp;= \sum_b W_{ij(b\beta)} \, e_{ijb}</span>
<span class="sd">    \end{aligned}</span>
<span class="sd">    $$</span>

<span class="sd">    , where $w_{ij(b\beta)}$ is an intemediate weight tensor for the</span>
<span class="sd">    radial basis functions, output by the `FFLayer`; the output channel is</span>
<span class="sd">    reshaped into two dimensions, where $b$ is the index for the basis function</span>
<span class="sd">    and $d$ is the index for output interaction.</span>


<span class="sd">    `n_nodes` specifies the number of nodes in the `FFLayer`. Note that the last</span>
<span class="sd">    element of n_nodes specifies the number of output channels after applying</span>
<span class="sd">    the basis function ($d$ instead of $bd$), i.e. the output dimension of</span>
<span class="sd">    FFLayer is `[n_pairs,n_nodes[-1]*n_basis]`, the output is then summed with</span>
<span class="sd">    the basis to form the output interaction.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_nodes</span><span class="o">=</span><span class="p">[</span><span class="mi">64</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            n_nodes (list of int): number of nodes to use</span>
<span class="sd">            **kwargs (dict): keyword arguments will be parsed to the feed forward layers</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">PILayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_nodes</span> <span class="o">=</span> <span class="n">n_nodes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kwargs</span> <span class="o">=</span> <span class="n">kwargs</span>

    <span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">shapes</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_basis</span> <span class="o">=</span> <span class="n">shapes</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">n_nodes_iter</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_nodes</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="n">n_nodes_iter</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">*=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_basis</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ff_layer</span> <span class="o">=</span> <span class="n">FFLayer</span><span class="p">(</span><span class="n">n_nodes_iter</span><span class="p">,</span> <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensors</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        PILayer take a list of three tensors as input:</span>

<span class="sd">        - ind_2: [sparse indices](layers.md#sparse-indices) of pairs with shape `(n_pairs, 2)`</span>
<span class="sd">        - prop: property tensor with shape `(n_atoms, n_prop)`</span>
<span class="sd">        - basis: interaction tensor with shape `(n_pairs, n_basis)`</span>

<span class="sd">        Args:</span>
<span class="sd">            tensors (list of tensors): list of `[ind_2, prop, basis]` tensors</span>

<span class="sd">        Returns:</span>
<span class="sd">            inter (tensor): interaction tensor with shape `(n_pairs, n_nodes[-1])`</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">ind_2</span><span class="p">,</span> <span class="n">prop</span><span class="p">,</span> <span class="n">basis</span> <span class="o">=</span> <span class="n">tensors</span>
        <span class="n">ind_i</span> <span class="o">=</span> <span class="n">ind_2</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>
        <span class="n">ind_j</span> <span class="o">=</span> <span class="n">ind_2</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span>
        <span class="n">prop_i</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">prop</span><span class="p">,</span> <span class="n">ind_i</span><span class="p">)</span>
        <span class="n">prop_j</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">prop</span><span class="p">,</span> <span class="n">ind_j</span><span class="p">)</span>

        <span class="n">inter</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">prop_i</span><span class="p">,</span> <span class="n">prop_j</span><span class="p">],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">inter</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ff_layer</span><span class="p">(</span><span class="n">inter</span><span class="p">)</span>
        <span class="n">inter</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">inter</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_nodes</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_basis</span><span class="p">])</span>
        <span class="n">inter</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;pcb,pb-&gt;pc&quot;</span><span class="p">,</span> <span class="n">inter</span><span class="p">,</span> <span class="n">basis</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">inter</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h2 id="pinn.networks.pinet.PILayer.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">n_nodes</span><span class="o">=</span><span class="p">[</span><span class="mi">64</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

</h2>


    <div class="doc doc-contents ">



<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>n_nodes</code>
            </td>
            <td>
                  <code>list of int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>number of nodes to use</p>
              </div>
            </td>
            <td>
                  <code>[64]</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>**kwargs</code>
            </td>
            <td>
                  <code><span title="dict">dict</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>keyword arguments will be parsed to the feed forward layers</p>
              </div>
            </td>
            <td>
                  <code>{}</code>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>pinn/networks/pinet.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_nodes</span><span class="o">=</span><span class="p">[</span><span class="mi">64</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        n_nodes (list of int): number of nodes to use</span>
<span class="sd">        **kwargs (dict): keyword arguments will be parsed to the feed forward layers</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">PILayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_nodes</span> <span class="o">=</span> <span class="n">n_nodes</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">kwargs</span> <span class="o">=</span> <span class="n">kwargs</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="pinn.networks.pinet.PILayer.build" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">build</span><span class="p">(</span><span class="n">shapes</span><span class="p">)</span></code>

</h2>


    <div class="doc doc-contents ">

        

            <details class="quote">
              <summary>Source code in <code>pinn/networks/pinet.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">shapes</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_basis</span> <span class="o">=</span> <span class="n">shapes</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">n_nodes_iter</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_nodes</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">n_nodes_iter</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">*=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_basis</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ff_layer</span> <span class="o">=</span> <span class="n">FFLayer</span><span class="p">(</span><span class="n">n_nodes_iter</span><span class="p">,</span> <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="pinn.networks.pinet.PILayer.call" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">call</span><span class="p">(</span><span class="n">tensors</span><span class="p">)</span></code>

</h2>


    <div class="doc doc-contents ">

        <p>PILayer take a list of three tensors as input:</p>
<ul>
<li>ind_2: <a href="../layers/#sparse-indices">sparse indices</a> of pairs with shape <code>(n_pairs, 2)</code></li>
<li>prop: property tensor with shape <code>(n_atoms, n_prop)</code></li>
<li>basis: interaction tensor with shape <code>(n_pairs, n_basis)</code></li>
</ul>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>tensors</code>
            </td>
            <td>
                  <code>list of tensors</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>list of <code>[ind_2, prop, basis]</code> tensors</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>inter</code></td>            <td>
                  <code><span title="tensor">tensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>interaction tensor with shape <code>(n_pairs, n_nodes[-1])</code></p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>pinn/networks/pinet.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensors</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    PILayer take a list of three tensors as input:</span>

<span class="sd">    - ind_2: [sparse indices](layers.md#sparse-indices) of pairs with shape `(n_pairs, 2)`</span>
<span class="sd">    - prop: property tensor with shape `(n_atoms, n_prop)`</span>
<span class="sd">    - basis: interaction tensor with shape `(n_pairs, n_basis)`</span>

<span class="sd">    Args:</span>
<span class="sd">        tensors (list of tensors): list of `[ind_2, prop, basis]` tensors</span>

<span class="sd">    Returns:</span>
<span class="sd">        inter (tensor): interaction tensor with shape `(n_pairs, n_nodes[-1])`</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">ind_2</span><span class="p">,</span> <span class="n">prop</span><span class="p">,</span> <span class="n">basis</span> <span class="o">=</span> <span class="n">tensors</span>
    <span class="n">ind_i</span> <span class="o">=</span> <span class="n">ind_2</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>
    <span class="n">ind_j</span> <span class="o">=</span> <span class="n">ind_2</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span>
    <span class="n">prop_i</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">prop</span><span class="p">,</span> <span class="n">ind_i</span><span class="p">)</span>
    <span class="n">prop_j</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">prop</span><span class="p">,</span> <span class="n">ind_j</span><span class="p">)</span>

    <span class="n">inter</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">prop_i</span><span class="p">,</span> <span class="n">prop_j</span><span class="p">],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">inter</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ff_layer</span><span class="p">(</span><span class="n">inter</span><span class="p">)</span>
    <span class="n">inter</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">inter</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_nodes</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_basis</span><span class="p">])</span>
    <span class="n">inter</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;pcb,pb-&gt;pc&quot;</span><span class="p">,</span> <span class="n">inter</span><span class="p">,</span> <span class="n">basis</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">inter</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div><h3 id="pinetiplayer">pinet.IPLayer</h3>


<div class="doc doc-object doc-class">



<a id="pinn.networks.pinet.IPLayer"></a>
    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="tensorflow.keras.layers.Layer">Layer</span></code></p>


        <p>The IPLayer transforms pairwise interactions to atomic properties</p>
<p>The IPLayer has no learnable variables and simply sums up the pairwise
interations. Thus the returned property has the same shape with the
input interaction, i.e.:</p>
<div class="arithmatex">\[
\begin{aligned}
\mathbb{P}_{i\gamma} = \mathrm{IPLayer}(\mathbb{I}_{ij\gamma}) = \sum_{j} \mathbb{I}_{ij\gamma}
\end{aligned}
\]</div>






              <details class="quote">
                <summary>Source code in <code>pinn/networks/pinet.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">IPLayer</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">R</span><span class="sd">&quot;&quot;&quot;The IPLayer transforms pairwise interactions to atomic properties</span>

<span class="sd">    The IPLayer has no learnable variables and simply sums up the pairwise</span>
<span class="sd">    interations. Thus the returned property has the same shape with the</span>
<span class="sd">    input interaction, i.e.:</span>

<span class="sd">    $$</span>
<span class="sd">    \begin{aligned}</span>
<span class="sd">    \mathbb{P}_{i\gamma} = \mathrm{IPLayer}(\mathbb{I}_{ij\gamma}) = \sum_{j} \mathbb{I}_{ij\gamma}</span>
<span class="sd">    \end{aligned}</span>
<span class="sd">    $$</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        IPLayer does not require any parameter, initialize as `IPLayer()`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">IPLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensors</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        IPLayer take a list of three tensors list as input:</span>

<span class="sd">        - ind_2: [sparse indices](layers.md#sparse-indices) of pairs with shape `(n_pairs, 2)`</span>
<span class="sd">        - prop: property tensor with shape `(n_atoms, n_prop)`</span>
<span class="sd">        - inter: interaction tensor with shape `(n_pairs, n_inter)`</span>

<span class="sd">        Args:</span>
<span class="sd">            tensors (list of tensor): list of [ind_2, prop, inter] tensors</span>

<span class="sd">        Returns:</span>
<span class="sd">            prop (tensor): new property tensor with shape `(n_atoms, n_inter)`</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">ind_2</span><span class="p">,</span> <span class="n">prop</span><span class="p">,</span> <span class="n">inter</span> <span class="o">=</span> <span class="n">tensors</span>
        <span class="n">n_atoms</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">prop</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">unsorted_segment_sum</span><span class="p">(</span><span class="n">inter</span><span class="p">,</span> <span class="n">ind_2</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">n_atoms</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h2 id="pinn.networks.pinet.IPLayer.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="fm">__init__</span><span class="p">()</span></code>

</h2>


    <div class="doc doc-contents ">

        <p>IPLayer does not require any parameter, initialize as <code>IPLayer()</code>.</p>

            <details class="quote">
              <summary>Source code in <code>pinn/networks/pinet.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    IPLayer does not require any parameter, initialize as `IPLayer()`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">IPLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="pinn.networks.pinet.IPLayer.call" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">call</span><span class="p">(</span><span class="n">tensors</span><span class="p">)</span></code>

</h2>


    <div class="doc doc-contents ">

        <p>IPLayer take a list of three tensors list as input:</p>
<ul>
<li>ind_2: <a href="../layers/#sparse-indices">sparse indices</a> of pairs with shape <code>(n_pairs, 2)</code></li>
<li>prop: property tensor with shape <code>(n_atoms, n_prop)</code></li>
<li>inter: interaction tensor with shape <code>(n_pairs, n_inter)</code></li>
</ul>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>tensors</code>
            </td>
            <td>
                  <code>list of tensor</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>list of [ind_2, prop, inter] tensors</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>prop</code></td>            <td>
                  <code><span title="tensor">tensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>new property tensor with shape <code>(n_atoms, n_inter)</code></p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>pinn/networks/pinet.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensors</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    IPLayer take a list of three tensors list as input:</span>

<span class="sd">    - ind_2: [sparse indices](layers.md#sparse-indices) of pairs with shape `(n_pairs, 2)`</span>
<span class="sd">    - prop: property tensor with shape `(n_atoms, n_prop)`</span>
<span class="sd">    - inter: interaction tensor with shape `(n_pairs, n_inter)`</span>

<span class="sd">    Args:</span>
<span class="sd">        tensors (list of tensor): list of [ind_2, prop, inter] tensors</span>

<span class="sd">    Returns:</span>
<span class="sd">        prop (tensor): new property tensor with shape `(n_atoms, n_inter)`</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">ind_2</span><span class="p">,</span> <span class="n">prop</span><span class="p">,</span> <span class="n">inter</span> <span class="o">=</span> <span class="n">tensors</span>
    <span class="n">n_atoms</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">prop</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">unsorted_segment_sum</span><span class="p">(</span><span class="n">inter</span><span class="p">,</span> <span class="n">ind_2</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">n_atoms</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div><h3 id="pinetresupdate">pinet.ResUpdate</h3>


<div class="doc doc-object doc-class">



<a id="pinn.networks.pinet.ResUpdate"></a>
    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="tensorflow.keras.layers.Layer">Layer</span></code></p>


        <p><code>ResUpdate</code> layer implements ResNet-like update of properties that
addresses vanishing/exploding gradient problems (see
<a href="https://arxiv.org/abs/1512.03385">arXiv:1512.03385</a>).</p>
<p>It takes two tensors (old and new) as input, the tensors should have the
same shape except for the last dimension, and a tensor with the shape of the
new tensor is always returned.</p>
<p>If shapes of the two tensors match, their sum is returned. If the two
tensors' shapes differ in the last dimension, the old tensor will be added
to the new after a learnable linear transformation that matches its shape to
the new tensor, i.e., according to the above flowchart:</p>
<div class="arithmatex">\[
\begin{aligned}
\mathbb{P}'_{i\gamma} &amp;= \mathrm{ResUpdate}(\mathbb{P}^{t}_{i\alpha},\mathbb{P}''_{i\gamma}) &amp; \\
  &amp;= \begin{cases}
       \mathbb{P}^{t}_{i\alpha} + \mathbb{P}''_{i\gamma} &amp; \textrm{, if } \mathrm{dim}(\mathbb{P}^{t}) = \mathrm{dim}(\mathbb{P}'')\\
       \sum_{\alpha} W_{\alpha\gamma} \, \mathbb{P}^{t}_{i\alpha} + \mathbb{P}''_{i\gamma} &amp; \textrm{, if } \mathrm{dim}(\mathbb{P}^{t}) \ne \mathrm{dim}(\mathbb{P}'')
     \end{cases}
\end{aligned}
\]</div>
<p>, where <span class="arithmatex">\(W_{\alpha\beta}\)</span> is a learnable weight matrix if needed.</p>
<p>In the PiNet architecture above, ResUpdate is only used to update the
properties after the <code>IPLayer</code>, when <code>ii_nodes[-1]==pp_nodes[-1]</code>, the
weight matrix is only necessary at <span class="arithmatex">\(t=0\)</span>.</p>






              <details class="quote">
                <summary>Source code in <code>pinn/networks/pinet.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">ResUpdate</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">R</span><span class="sd">&quot;&quot;&quot;`ResUpdate` layer implements ResNet-like update of properties that</span>
<span class="sd">    addresses vanishing/exploding gradient problems (see</span>
<span class="sd">    [arXiv:1512.03385](https://arxiv.org/abs/1512.03385)).</span>

<span class="sd">    It takes two tensors (old and new) as input, the tensors should have the</span>
<span class="sd">    same shape except for the last dimension, and a tensor with the shape of the</span>
<span class="sd">    new tensor is always returned.</span>

<span class="sd">    If shapes of the two tensors match, their sum is returned. If the two</span>
<span class="sd">    tensors&#39; shapes differ in the last dimension, the old tensor will be added</span>
<span class="sd">    to the new after a learnable linear transformation that matches its shape to</span>
<span class="sd">    the new tensor, i.e., according to the above flowchart:</span>

<span class="sd">    $$</span>
<span class="sd">    \begin{aligned}</span>
<span class="sd">    \mathbb{P}&#39;_{i\gamma} &amp;= \mathrm{ResUpdate}(\mathbb{P}^{t}_{i\alpha},\mathbb{P}&#39;&#39;_{i\gamma}) &amp; \\</span>
<span class="sd">      &amp;= \begin{cases}</span>
<span class="sd">           \mathbb{P}^{t}_{i\alpha} + \mathbb{P}&#39;&#39;_{i\gamma} &amp; \textrm{, if } \mathrm{dim}(\mathbb{P}^{t}) = \mathrm{dim}(\mathbb{P}&#39;&#39;)\\</span>
<span class="sd">           \sum_{\alpha} W_{\alpha\gamma} \, \mathbb{P}^{t}_{i\alpha} + \mathbb{P}&#39;&#39;_{i\gamma} &amp; \textrm{, if } \mathrm{dim}(\mathbb{P}^{t}) \ne \mathrm{dim}(\mathbb{P}&#39;&#39;)</span>
<span class="sd">         \end{cases}</span>
<span class="sd">    \end{aligned}</span>
<span class="sd">    $$</span>

<span class="sd">    , where $W_{\alpha\beta}$ is a learnable weight matrix if needed.</span>

<span class="sd">    In the PiNet architecture above, ResUpdate is only used to update the</span>
<span class="sd">    properties after the `IPLayer`, when `ii_nodes[-1]==pp_nodes[-1]`, the</span>
<span class="sd">    weight matrix is only necessary at $t=0$.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        ResUpdate does not require any parameter, initialize as `ResUpdate()`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ResUpdate</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">shapes</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;&quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">shapes</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">shapes</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span>
        <span class="k">if</span> <span class="n">shapes</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">shapes</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">transform</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">transform</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span>
                <span class="n">shapes</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="kc">None</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensors</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">           tensors (list of tensors): two tensors with matching shapes expect the last dimension</span>

<span class="sd">        Returns:</span>
<span class="sd">           tensor (tensor): updated tensor with the same shape as the second input tensor</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">old</span><span class="p">,</span> <span class="n">new</span> <span class="o">=</span> <span class="n">tensors</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">old</span><span class="p">)</span> <span class="o">+</span> <span class="n">new</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h2 id="pinn.networks.pinet.ResUpdate.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="fm">__init__</span><span class="p">()</span></code>

</h2>


    <div class="doc doc-contents ">

        <p>ResUpdate does not require any parameter, initialize as <code>ResUpdate()</code>.</p>

            <details class="quote">
              <summary>Source code in <code>pinn/networks/pinet.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    ResUpdate does not require any parameter, initialize as `ResUpdate()`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">ResUpdate</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="pinn.networks.pinet.ResUpdate.build" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">build</span><span class="p">(</span><span class="n">shapes</span><span class="p">)</span></code>

</h2>


    <div class="doc doc-contents ">

        

            <details class="quote">
              <summary>Source code in <code>pinn/networks/pinet.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">shapes</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;&quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">shapes</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">shapes</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span>
    <span class="k">if</span> <span class="n">shapes</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">shapes</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">transform</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">transform</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span>
            <span class="n">shapes</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="kc">None</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="pinn.networks.pinet.ResUpdate.call" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">call</span><span class="p">(</span><span class="n">tensors</span><span class="p">)</span></code>

</h2>


    <div class="doc doc-contents ">



<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>tensors</code>
            </td>
            <td>
                  <code>list of tensors</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>two tensors with matching shapes expect the last dimension</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>tensor</code></td>            <td>
                  <code><span title="tensor">tensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>updated tensor with the same shape as the second input tensor</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>pinn/networks/pinet.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensors</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">       tensors (list of tensors): two tensors with matching shapes expect the last dimension</span>

<span class="sd">    Returns:</span>
<span class="sd">       tensor (tensor): updated tensor with the same shape as the second input tensor</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">old</span><span class="p">,</span> <span class="n">new</span> <span class="o">=</span> <span class="n">tensors</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">old</span><span class="p">)</span> <span class="o">+</span> <span class="n">new</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div><h3 id="pinetoutlayer">pinet.OutLayer</h3>


<div class="doc doc-object doc-class">



<a id="pinn.networks.pinet.OutLayer"></a>
    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="tensorflow.keras.layers.Layer">Layer</span></code></p>


        <p><code>OutLayer</code> updates the network output with a <code>FFLayer</code> layer, where the
<code>out_units</code> controls the dimension of outputs. In addition to the <code>FFLayer</code>
specified by <code>n_nodes</code>, the <code>OutLayer</code> has one additional linear biasless
layer that scales the outputs, specified by <code>out_units</code>.</p>






              <details class="quote">
                <summary>Source code in <code>pinn/networks/pinet.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">OutLayer</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;`OutLayer` updates the network output with a `FFLayer` layer, where the</span>
<span class="sd">    `out_units` controls the dimension of outputs. In addition to the `FFLayer`</span>
<span class="sd">    specified by `n_nodes`, the `OutLayer` has one additional linear biasless</span>
<span class="sd">    layer that scales the outputs, specified by `out_units`.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_nodes</span><span class="p">,</span> <span class="n">out_units</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            n_nodes (list): dimension of the hidden layers</span>
<span class="sd">            out_units (int): dimension of the output units</span>
<span class="sd">            **kwargs (dict): options to be parsed to dense layers</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">OutLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_units</span> <span class="o">=</span> <span class="n">out_units</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ff_layer</span> <span class="o">=</span> <span class="n">FFLayer</span><span class="p">(</span><span class="n">n_nodes</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_units</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span>
            <span class="n">out_units</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensors</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        OutLayer takes a list of three tensors as input:</span>

<span class="sd">        - ind_1: [sparse indices](layers.md#sparse-indices) of atoms with shape `(n_atoms, 2)`</span>
<span class="sd">        - prop: property tensor with shape `(n_atoms, n_prop)`</span>
<span class="sd">        - prev_output:  previous output with shape `(n_atoms, out_units)`</span>

<span class="sd">        Args:</span>
<span class="sd">            tensors (list of tensors): list of [ind_1, prop, prev_output] tensors</span>

<span class="sd">        Returns:</span>
<span class="sd">            output (tensor): an updated output tensor with shape `(n_atoms, out_units)`</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">ind_1</span><span class="p">,</span> <span class="n">prop</span><span class="p">,</span> <span class="n">prev_output</span> <span class="o">=</span> <span class="n">tensors</span>
        <span class="n">prop</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ff_layer</span><span class="p">(</span><span class="n">prop</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_units</span><span class="p">(</span><span class="n">prop</span><span class="p">)</span> <span class="o">+</span> <span class="n">prev_output</span>
        <span class="k">return</span> <span class="n">output</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h2 id="pinn.networks.pinet.OutLayer.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">n_nodes</span><span class="p">,</span> <span class="n">out_units</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

</h2>


    <div class="doc doc-contents ">



<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>n_nodes</code>
            </td>
            <td>
                  <code><span title="list">list</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>dimension of the hidden layers</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>out_units</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>dimension of the output units</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>**kwargs</code>
            </td>
            <td>
                  <code><span title="dict">dict</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>options to be parsed to dense layers</p>
              </div>
            </td>
            <td>
                  <code>{}</code>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>pinn/networks/pinet.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_nodes</span><span class="p">,</span> <span class="n">out_units</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        n_nodes (list): dimension of the hidden layers</span>
<span class="sd">        out_units (int): dimension of the output units</span>
<span class="sd">        **kwargs (dict): options to be parsed to dense layers</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">OutLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">out_units</span> <span class="o">=</span> <span class="n">out_units</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ff_layer</span> <span class="o">=</span> <span class="n">FFLayer</span><span class="p">(</span><span class="n">n_nodes</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">out_units</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span>
        <span class="n">out_units</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="pinn.networks.pinet.OutLayer.call" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">call</span><span class="p">(</span><span class="n">tensors</span><span class="p">)</span></code>

</h2>


    <div class="doc doc-contents ">

        <p>OutLayer takes a list of three tensors as input:</p>
<ul>
<li>ind_1: <a href="../layers/#sparse-indices">sparse indices</a> of atoms with shape <code>(n_atoms, 2)</code></li>
<li>prop: property tensor with shape <code>(n_atoms, n_prop)</code></li>
<li>prev_output:  previous output with shape <code>(n_atoms, out_units)</code></li>
</ul>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>tensors</code>
            </td>
            <td>
                  <code>list of tensors</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>list of [ind_1, prop, prev_output] tensors</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>output</code></td>            <td>
                  <code><span title="tensor">tensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>an updated output tensor with shape <code>(n_atoms, out_units)</code></p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>pinn/networks/pinet.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensors</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    OutLayer takes a list of three tensors as input:</span>

<span class="sd">    - ind_1: [sparse indices](layers.md#sparse-indices) of atoms with shape `(n_atoms, 2)`</span>
<span class="sd">    - prop: property tensor with shape `(n_atoms, n_prop)`</span>
<span class="sd">    - prev_output:  previous output with shape `(n_atoms, out_units)`</span>

<span class="sd">    Args:</span>
<span class="sd">        tensors (list of tensors): list of [ind_1, prop, prev_output] tensors</span>

<span class="sd">    Returns:</span>
<span class="sd">        output (tensor): an updated output tensor with shape `(n_atoms, out_units)`</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">ind_1</span><span class="p">,</span> <span class="n">prop</span><span class="p">,</span> <span class="n">prev_output</span> <span class="o">=</span> <span class="n">tensors</span>
    <span class="n">prop</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ff_layer</span><span class="p">(</span><span class="n">prop</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_units</span><span class="p">(</span><span class="n">prop</span><span class="p">)</span> <span class="o">+</span> <span class="n">prev_output</span>
    <span class="k">return</span> <span class="n">output</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div><div class="footnote">
<hr />
<ol>
<li id="fn:1">
<p><sup>1</sup> Y. Shao, M. Hellstrm, P.D. Mitev, L. Knijff, and C. Zhang, <a href="https://doi.org/10.1021/acs.jcim.9b00994">PiNN: A python library for building atomic neural networks of molecules and materials</a>, J. Chem. Inf. Model. <strong>60</strong>(3), 11841193 (2020).&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>
    </div>

  </div>

  <div class="page">
    <div class="page-prev">
      
      <a href="../layers/" title="Layers"><span></span> Previous</a>
      
    </div>
    <div class="page-next">
      
      <a href="../pinet2/" title="PiNet2" />Next <span></span></a>
      
    </div>
  </div>

  <div class="footer">
    Built with <a href="https://www.mkdocs.org">mkdocs</a> and the <a href="https://github.com/yqshao/mkdocs-flux-theme">flux</a> theme.
  </div>
</body>
</html>