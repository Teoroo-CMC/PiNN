{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building machine learning potentials (MLPs) for liquid water [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Teoroo-CMC/PiNN/blob/pre-release/docs/notebooks/Build_MLP_Water.ipynb)\n",
    "\n",
    "\n",
    "This notebook showcases the usage of PiNN with a practical problem of building machine learning potentials for liquid water on the H2O(l)-revPBE0-D3 dataset.\n",
    "It serves as a basic test and demonstration of the workflow with PiNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PiNN\n",
    "!pip install git+https://github.com/Teoroo-CMC/PiNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the H2O(l)-revPBE0-D3 dataset\n",
    "\n",
    "The H2O(l)-revPBE0-D3 dataset was published by Cheng et al ([Proc. Nat. Acad. Sci. 2019, 116, 1110-1115](https://doi.org/10.1073/pnas.1815117116)). It comprises 1593 liquid water configurations involving 64 molecules with the energies and forces computed by DFT at the revPBE0-D3 level and presents an excellent example to test MLPs in the interpolation regime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step1: download the dataset from https://github.com/BingqingCheng/ab-initio-thermodynamics-of-water/blob/master/training-set/input.data\n",
    "\n",
    "# step2: create the project directory\n",
    "!mkdir MLP_Water\n",
    "\n",
    "!mv input.data MLP_Water"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the model architecture and parameters\n",
    "\n",
    "Three different model architectures are supplied in the PiNN package, i.e., Behler−Parrinello neural network (BPNN, [Phys. Rev. Lett. 2007, 98, 146401](https://doi.org/10.1103/PhysRevLett.98.146401)), PiNet ([J. Chem. Inf. Model. 2020, 60, 1184−1193](https://doi.org/10.1021/acs.jcim.9b00994)), and PiNet2-P3. The BPNN and PiNet only involve invariant features, whereas PiNet2-P3 incorporates equivariant information as well. Generally speaking, the training loss of PiNet2-P3 is lower than that of BPNN/PiNet. Before training the MLPs, it is essential to define the model architecture and set corresponding parameters. In this case, we will use PiNet2-P3 as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step1: copy a sample configuration file from PiNN package\n",
    "!cp ${Path_to_PiNN}/inputs/pinet2-pot.yml ./MLP_Water\n",
    "\n",
    "# step2: modify the architecture and parameters, the following is an example\n",
    "#     model:\n",
    "#       name: potential_model\n",
    "#       params:\n",
    "#         e_loss_multiplier: 10.0\n",
    "#         e_scale: 27.2114\n",
    "#         e_unit: 27.2114\n",
    "#         f_loss_multiplier: 100.0\n",
    "#         log_e_per_atom: true\n",
    "#         use_e_per_atom: false\n",
    "#         use_force: true\n",
    "#     network:\n",
    "#       name: PiNet2\n",
    "#       params:\n",
    "#         atom_types:\n",
    "#         - 1\n",
    "#         - 8\n",
    "#         basis_type: gaussian\n",
    "#         depth: 5\n",
    "#         ii_nodes:\n",
    "#         - 16\n",
    "#         - 16\n",
    "#         - 16\n",
    "#         - 16\n",
    "#         n_basis: 10\n",
    "#         out_nodes:\n",
    "#         - 16\n",
    "#         pi_nodes:\n",
    "#         - 16\n",
    "#         pp_nodes:\n",
    "#         - 16\n",
    "#         - 16\n",
    "#         - 16\n",
    "#         - 16\n",
    "#         rank: 3\n",
    "#         rc: 6.0\n",
    "#         weighted: false\n",
    "#     optimizer:\n",
    "#       class_name: Adam\n",
    "#       config:\n",
    "#         global_clipnorm: 0.01\n",
    "#         learning_rate:\n",
    "#           class_name: ExponentialDecay\n",
    "#           config:\n",
    "#             decay_rate: 0.994\n",
    "#             decay_steps: 100000\n",
    "#             initial_learning_rate: 5.0e-05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the MLP models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "from pinn import get_model, get_network\n",
    "from pinn.utils import init_params\n",
    "from pinn.io import load_runner, write_tfrecord, load_tfrecord, sparse_batch\n",
    "from tensorflow.python.lib.io.file_io import FileIO\n",
    "from tempfile import mkdtemp, mkstemp\n",
    "\n",
    "random_seed = 1 # random seed for data splitting\n",
    "num_train_steps = 5000000 # total number of training steps\n",
    "eval_per_num_steps = 5000 # evaluate the models every eval_per_num_steps steps\n",
    "batch_size = 1 # batch size\n",
    "nmax_ckpts = 1 # max number of checkpoint file\n",
    "nckpt_every = 5000 # save the checkpoint file every nckpt_every steps\n",
    "nlog_every = 5000 # write the log file every nlog_every steps\n",
    "num_shuffle_buffer = 1000\n",
    "bool_shuffle = True\n",
    "bool_preprocess = True # turn on/off the preprocess of dataset\n",
    "bool_cache = True\n",
    "bool_early_stop = False # turn on/off the early stop\n",
    "\n",
    "# set the GPU\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "# turn off the specific warning of tensorflow\n",
    "index_warning = 'Converting sparse IndexedSlices'\n",
    "warnings.filterwarnings('ignore', index_warning)\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "project_dir = \"${Parent_Path_to_Project}MLP_Water/\"\n",
    "\n",
    "# split training and test sets\n",
    "runner_dataset_file = project_dir + \"input.data\"\n",
    "dataset = load_runner(runner_dataset_file, splits={'train':8, 'vali':2}, shuffle=bool_shuffle, seed=random_seed)\n",
    "write_tfrecord(project_dir + 'train.yml', dataset['train'])\n",
    "write_tfrecord(project_dir + 'vali.yml', dataset['vali'])\n",
    "\n",
    "# get model parameters\n",
    "params = {}\n",
    "with FileIO(project_dir + \"pinet2-pot.yml\", 'r') as f:\n",
    "    params = yaml.load(f, Loader=yaml.Loader)\n",
    "\n",
    "molde_dir = project_dir + \"PiNet2_rPBE0_seed%d\"%(random_seed)\n",
    "params['model_dir'] = molde_dir\n",
    "\n",
    "# initial some parameters (e.g., e_dress)\n",
    "ds = load_tfrecord(project_dir + \"train.yml\")\n",
    "init_params(params, ds)\n",
    "\n",
    "scratch_dir = None\n",
    "if scratch_dir is not None:\n",
    "    scratch_dir = mkdtemp(prefix='pinn', dir=scratch_dir)\n",
    "def _dataset_fn(fname):\n",
    "    dataset = load_tfrecord(fname)\n",
    "    if batch_size is not None:\n",
    "        dataset = dataset.apply(sparse_batch(batch_size))\n",
    "    if bool_preprocess:\n",
    "        def pre_fn(tensors):\n",
    "            with tf.name_scope(\"PRE\") as scope:\n",
    "                network = get_network(params['network'])\n",
    "                tensors = network.preprocess(tensors)\n",
    "            return tensors\n",
    "        dataset = dataset.map(pre_fn)\n",
    "    if bool_cache:\n",
    "        if scratch_dir is not None:\n",
    "            cache_dir = mkstemp(dir=scratch_dir)\n",
    "        else:\n",
    "            cache_dir = ''\n",
    "        dataset = dataset.cache(cache_dir)\n",
    "    return dataset\n",
    "\n",
    "train_fn = lambda: _dataset_fn(project_dir + 'train.yml').repeat().shuffle(num_shuffle_buffer)\n",
    "eval_fn = lambda: _dataset_fn(project_dir + 'vali.yml')\n",
    "config = tf.estimator.RunConfig(keep_checkpoint_max=nmax_ckpts,\n",
    "                                log_step_count_steps=nlog_every,\n",
    "                                save_summary_steps=nlog_every,\n",
    "                                save_checkpoints_steps=nckpt_every)\n",
    "\n",
    "model = get_model(params, config=config)\n",
    "\n",
    "if bool_early_stop:\n",
    "    early_stop = \"loss:1000\"\n",
    "    stops = {s.split(':')[0]: float(s.split(':')[1])\n",
    "             for s in early_stop.split(',')}\n",
    "    hooks = [tf.estimator.experimental.stop_if_no_decrease_hook(\n",
    "        model, k, v) for k,v in stops.items()]\n",
    "else:\n",
    "    hooks=None\n",
    "\n",
    "train_spec = tf.estimator.TrainSpec(input_fn=train_fn, max_steps=num_train_steps, hooks=hooks)\n",
    "eval_spec  = tf.estimator.EvalSpec(input_fn=eval_fn, steps=eval_per_num_steps)\n",
    "tf.estimator.train_and_evaluate(model, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def log(log_dir, tag, fmt):\n",
    "    from glob import glob\n",
    "    from sys import stdout\n",
    "    from warnings import warn\n",
    "    from itertools import chain\n",
    "    from tensorboard.backend.event_processing.event_file_loader import LegacyEventFileLoader\n",
    "    files = sorted(glob(f'{log_dir}/events.out.*'), key=os.path.getmtime)\n",
    "    logs = {}\n",
    "    events = chain(*[LegacyEventFileLoader(log).Load() for log in files])\n",
    "    for event in events:\n",
    "        for v in event.summary.value:\n",
    "            if tag not in v.tag:\n",
    "                continue\n",
    "            if v.tag not in logs.keys():\n",
    "                logs[v.tag] = []\n",
    "            logs[v.tag].append([event.step, v.simple_value])\n",
    "    logs = {k: np.array(v) for k,v in logs.items()}\n",
    "    keys = sorted(list(logs.keys()))\n",
    "    steps = [logs[k][:,0] for k in keys]\n",
    "    data = [logs[k][:,1] for k in keys]\n",
    "    steps, rows = np.unique(np.concatenate(steps), return_inverse=True)\n",
    "    \n",
    "    return (steps,data,keys)\n",
    "\n",
    "log_dir_train = molde_dir + \"/\"\n",
    "train_loss = log(log_dir_train,'RMSE', '%14.6e ')\n",
    "\n",
    "log_dir_eval = molde_dir + \"/eval\"\n",
    "eval_loss = log(log_dir_eval,'RMSE', '%14.6e ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_metrics = len(train_loss[2])\n",
    "\n",
    "fig, axs = plt.subplots(1, num_metrics, figsize=(15, 4))\n",
    "\n",
    "for index in range(0, num_metrics):\n",
    "    \n",
    "    axs[index].set_xlabel(r'Step')\n",
    "    axs[index].set_ylabel(train_loss[2][index])\n",
    "    axs[index].set_xlim(1E6, 5E6)\n",
    "    \n",
    "    max_value = np.max(eval_loss[1][index])\n",
    "    min_value = np.min(eval_loss[1][index])\n",
    "    axs[index].set_ylim(min_value, max_value)\n",
    "    \n",
    "    axs[index].plot(train_loss[0], train_loss[1][index], color='black', label=\"Training\")\n",
    "    axs[index].plot(eval_loss[0], eval_loss[1][index], color='red', label=\"Validation\")\n",
    "    axs[index].legend(edgecolor='none', loc='best')\n",
    "\n",
    "plt.subplots_adjust(hspace=500)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run molecular dynamics with ASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinn\n",
    "from ase import units\n",
    "from ase.io import read,write\n",
    "from ase.io.trajectory import Trajectory\n",
    "from ase.md import MDLogger\n",
    "from ase.md.velocitydistribution import MaxwellBoltzmannDistribution\n",
    "from ase.md.nptberendsen import NPTBerendsen\n",
    "from ase.md.nvtberendsen import NVTBerendsen\n",
    "\n",
    "setup = {\n",
    "        'ensemble': 'nvt',  # ensemble\n",
    "        'T': 330.0,  # temperature\n",
    "        't': 200000,  # time in fs\n",
    "        'dt': 0.5,  # timestep in fs\n",
    "        'taut': 10,  # thermostat damping in steps\n",
    "        'taup': 1000,  # barastat dampling in steps\n",
    "        'log-every': 100,  # log interval in steps\n",
    "        'pressure': 1,  # pressure in bar\n",
    "        'compressibility': 4.57e-4,  # compressibility in bar^{-1}\n",
    "    }\n",
    "\n",
    "ensemble=setup['ensemble']\n",
    "T = float(setup['T'])\n",
    "t=float(setup['t'])*units.fs\n",
    "dt=float(setup['dt'])*units.fs\n",
    "taut=int(setup['taut'])\n",
    "taup=int(setup['taup'])\n",
    "every=int(setup['log-every'])\n",
    "pressure=float(setup['pressure'])\n",
    "compressibility=float(setup['compressibility'])\n",
    "\n",
    "# get calculator\n",
    "calc = pinn.get_calc(molde_dir + \"/params.yml\")\n",
    "\n",
    "# read initial structure\n",
    "init_structure_file = project_dir + \"water.xyz\"\n",
    "atoms = read(init_structure_file)\n",
    "atoms.set_calculator(calc)\n",
    "\n",
    "# assign initial velocities (i.e., momenta)\n",
    "MaxwellBoltzmannDistribution(atoms, T * units.kB)\n",
    "\n",
    "if ensemble == 'npt':\n",
    "    dyn = NPTBerendsen(atoms, timestep=dt, temperature=T, pressure=pressure,\n",
    "                  taut=dt * taut, taup=dt * taup, compressibility=compressibility)\n",
    "if ensemble == 'nvt':\n",
    "    dyn = NVTBerendsen(atoms, timestep=dt, temperature=T, taut=dt * taut)\n",
    "\n",
    "log_file = project_dir + \"asemd.log\"\n",
    "dyn.attach(\n",
    "    MDLogger(dyn, atoms,log_file,stress=True, mode=\"w\"),\n",
    "    interval=int(50*units.fs/dt))\n",
    "\n",
    "traj_file = project_dir + \"asemd.traj\"\n",
    "dyn.attach(\n",
    "    Trajectory(traj_file, 'w', atoms).write,\n",
    "    interval=int(50*units.fs/dt))\n",
    "\n",
    "# run simulations\n",
    "dyn.run(int(t/dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the trajectory\n",
    "\n",
    "Once the trajectory has been acquired, you can perform various analyses based on it, such as calculating the radial distribution function (RDF) for specific pairs of atoms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from ase.io import iread\n",
    "\n",
    "r_min = 0.0\n",
    "r_max = 6.0\n",
    "bin_num = 201\n",
    "hist = 0\n",
    "\n",
    "bins = np.linspace(r_min,r_max,bin_num)\n",
    "r_mid = (bins[1:] + bins[:-1])/2\n",
    "bin_vol = (bins[1:]**3 - bins[:-1]**3)*4*np.pi/3\n",
    "\n",
    "from ase.io.trajectory import Trajectory\n",
    "trajObj = Trajectory(traj_file)\n",
    "start_fram_index = int(len(trajObj) / 2)\n",
    "\n",
    "count = 0\n",
    "traj = iread(traj_file, index=slice(start_fram_index, None))\n",
    "for data in traj:\n",
    "    \n",
    "    count += 1\n",
    "    \n",
    "    listIndex1 = [idx for idx in range(len(data)) if data[idx].symbol == \"O\"]\n",
    "    index_ij = np.array(list(itertools.combinations(listIndex1, 2)))\n",
    "    \n",
    "    data.wrap()\n",
    "\n",
    "    diff = data.positions[index_ij[:, 0]] - data.positions[index_ij[:, 1]]\n",
    "    listCellLen = [data.cell[0][0], data.cell[1][1], data.cell[2][2]]\n",
    "    diff = diff - np.rint(diff / listCellLen) * listCellLen\n",
    "    dist = np.linalg.norm(diff, axis=1)\n",
    "\n",
    "    h, edges = np.histogram(dist, bins)\n",
    "\n",
    "    # normalize by the number density\n",
    "    rho = dist.shape[0] / data.get_volume()\n",
    "    hist += h / bin_vol / rho\n",
    "\n",
    "rdf = hist/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 1, figsize=(4, 4))\n",
    "\n",
    "axs.set_xlabel(r'$r \\, (\\mathrm{\\AA}$)')\n",
    "axs.set_ylabel(r'$g_\\mathrm{OO}(r)$')\n",
    "axs.set_xlim(2, 6)\n",
    "\n",
    "max_value = np.max(rdf)\n",
    "min_value = np.min(rdf)\n",
    "axs.set_ylim(min_value, max_value+0.2)\n",
    "\n",
    "axs.plot(r_mid, rdf, color='black')\n",
    "\n",
    "plt.subplots_adjust(hspace=500)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Nextflow\n",
    "\n",
    "When there is a need for batch model training and MD running jobs, incorporating the aforementioned process into a Nextflow script is a good choice. You can find more information of Nextflow from https://www.nextflow.io/docs/latest/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env nextflow\n",
    "nextflow.enable.dsl=2\n",
    "\n",
    "// Input files\n",
    "params.inputs = './inputs/pinn-adam/*.yml'\n",
    "params.dataset = './datasets/*.data'\n",
    "// Other parameters\n",
    "params.seeds = 1       // number of seeds to initialize the training\n",
    "params.steps = 5000000     // max number of steps\n",
    "params.batch = 1           // batch size\n",
    "params.ckpts = 1           // number of ckpts to keep, 0 -> all\n",
    "params.log_every = 50   // log/summary frequency\n",
    "params.ckpt_every = 50 // checkpoint frequency\n",
    "\n",
    "model_config = Channel.fromPath(params.inputs)\n",
    "    .combine(Channel.fromPath(params.dataset))\n",
    "    .combine(Channel.value(params.steps))\n",
    "    .combine(Channel.of(1..params.seeds))\n",
    "\n",
    "workflow {\n",
    "    pinn_train(model_config)\n",
    "}\n",
    "\n",
    "def shorten(x) {sprintf('%.5E', (double) x).replaceAll(/\\.?0*E/, 'E').replaceAll(/E\\+0*/, 'E')}\n",
    "\n",
    "process pinn_train {\n",
    "\n",
    "    publishDir 'models/', mode: 'link'\n",
    "    tag \"$ds.baseName-$input.baseName-B${params.batch}-${shorten(step)}-$seed\"\n",
    "    label 'pinn'\n",
    "\n",
    "    input:\n",
    "    tuple (file(input), file(ds), val(step), val(seed))\n",
    "\n",
    "    output:\n",
    "    path \"$ds.baseName-$input.baseName-B${params.batch}-${shorten(step)}-$seed\", type:'dir'\n",
    "\n",
    "    \"\"\"\n",
    "    pinn convert $ds -o 'train:8,eval:2' --seed $seed\n",
    "    pinn train $input -d \"$ds.baseName-$input.baseName-B${params.batch}-${shorten(step)}-$seed\"\\\n",
    "        --train-steps $params.steps --log-every $params.log_every --ckpt-every $params.ckpt_every\\\n",
    "        --batch $params.batch --max-ckpts $params.ckpts --shuffle-buffer 1000 --init\n",
    "    \"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch MD running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env nextflow\n",
    "nextflow.enable.dsl=2\n",
    "\n",
    "// Input files\n",
    "params.md_init = \"./inputs/water.xyz\"\n",
    "params.model = './Model_Path/PiNet2*/'\n",
    "// Other parameters\n",
    "params.md_ps = 200         // length of NVT simulation\n",
    "\n",
    "model_config = Channel.fromPath(params.md_init)\n",
    "               .combine(Channel.fromPath(params.model, type: 'dir'))\n",
    "               .combine(Channel.of(300)) # temperature\n",
    "\n",
    "workflow {\n",
    "    pinn_nvt(model_config)\n",
    "}\n",
    "\n",
    "def shorten(x) {sprintf('%.5E', (double) x).replaceAll(/\\.?0*E/, 'E').replaceAll(/E\\+0*/, 'E')}\n",
    "\n",
    "process pinn_nvt {\n",
    "    publishDir \"mds/$pinn_model.baseName\", mode: 'link'\n",
    "    tag \"$pinn_model.baseName\"\n",
    "    label 'pinn'\n",
    "\n",
    "    input:\n",
    "    tuple (file(md_geo),file(pinn_model),val(temp))\n",
    "\n",
    "    output:\n",
    "    file \"NVT-${temp}K-${params.md_ps}ps-${md_geo.simpleName}.log\"\n",
    "    file \"NVT-${temp}K-${params.md_ps}ps-${md_geo.simpleName}.traj\"\n",
    "\n",
    "    \"\"\"\n",
    "    #!/usr/bin/env python3\n",
    "    import pinn\n",
    "    import tensorflow as tf\n",
    "    from ase import units\n",
    "    from ase.io import read\n",
    "    from ase.io.trajectory import Trajectory\n",
    "    from ase.md import MDLogger\n",
    "    from ase.md.velocitydistribution import MaxwellBoltzmannDistribution\n",
    "    from ase.md.nvtberendsen import NVTBerendsen\n",
    "\n",
    "    dTemp = $temp\n",
    "    calc = pinn.get_calc(\"$pinn_model\")\n",
    "    atoms = read(\"$md_geo\")\n",
    "    atoms.set_calculator(calc)\n",
    "    MaxwellBoltzmannDistribution(atoms, dTemp*units.kB)\n",
    "    dt = 0.5 * units.fs\n",
    "    dyn = NVTBerendsen(atoms, timestep=dt, temperature=dTemp, taut=dt*100)\n",
    "\n",
    "    dyn.attach(\n",
    "        MDLogger(dyn, atoms, 'NVT-${temp}K-${params.md_ps}ps-${md_geo.simpleName}.log',stress=True, mode=\"w\"),\n",
    "        interval=int(100*units.fs/dt))\n",
    "    dyn.attach(\n",
    "        Trajectory('NVT-${temp}K-${params.md_ps}ps-${md_geo.simpleName}.traj', 'w', atoms).write,\n",
    "        interval=int(100*units.fs/dt))\n",
    "    for i in range($params.md_ps):\n",
    "        dyn.run(int(1e3*units.fs/dt))\n",
    " \"\"\"\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
